{"config":{"lang":["it"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ReCaS \u2013 Container &amp; Workflow","text":"<p>In questa documentazione trovi una guida pratica per usare immagini Apptainer/Singularity con HTCondor e container Docker con Kubernetes sul cluster ReCaS, con esempi gi\u00e0 pronti per Geant4/ROOT/Python.</p> <p>Usa il menu a sinistra per aprire la guida su HTCondor + Apptainer o quella su Kubernetes, a seconda del workflow che ti interessa.</p>"},{"location":"apptainer_condor/","title":"Uso di immagini Apptainer/Singularity con HTCondor su ReCaS","text":""},{"location":"apptainer_condor/#introduzione","title":"Introduzione","text":"<p>Questa guida descrive in modo operativo come usare immagini Apptainer/Singularity (file <code>.sif</code>) insieme a HTCondor sul cluster ReCaS. L\u2019idea di fondo \u00e8 che ci sia almeno un utente \u201cmanutentore\u201d (che chiameremo <code>alice</code>) che possa costruire immagini Docker su una macchina dedicata, convertirle in immagini Apptainer/Singularity e metterle a disposizione di tutti in una posizione condivisa su lustre. Gli altri utenti (ad esempio <code>bob</code>) non devono occuparsi della parte Docker: si limitano a usare le immagini <code>.sif</code> gi\u00e0 pronte all\u2019interno dei job Condor, tramite symlink verso la directory condivisa di <code>alice</code>.</p> <p>Nel seguito useremo come esempio un utente chiamato <code>bob</code> per i job Condor, mentre <code>alice</code> rappresenter\u00e0 l\u2019utente che ospita le immagini condivise. Per rendere gli esempi concreti, si assume che siano gi\u00e0 presenti due immagini Apptainer nella directory condivisa di <code>alice</code>:</p> Bash<pre><code>/lustrehome/alice/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif\n/lustrehome/alice/apptainer_images/G4_v11.3.1.sif\n</code></pre> <p>Queste immagini contengono Ubuntu 24.04, Geant4, ROOT, Python3 con numpy e matplotlib. In particolare l\u2019immagine <code>G4_v10.6.3_NOMULTITHREAD.sif</code> ha il multithreading disattivato per Geant4 ed \u00e8 adatta ad applicazioni che richiedono esecuzione single-thread.</p> <p>La sezione Concetti di base: container, Apptainer e HTCondor introduce i concetti di base su container, Apptainer e HTCondor, mentre la sezione Organizzazione delle directory propone una convenzione semplice per organizzare le directory su lustre dal punto di vista di <code>bob</code>.</p> <p>La sezione Esempi illustra cinque esempi completi di utilizzo: test dell\u2019immagine, build dell\u2019esempio B5 di Geant4, run di B5, build+run in un unico job e un caso reale con una simulazione Geant di una tile scintillante tra due piani di fibre WLS (CsI-WLS) e Python.</p> <p>La sezione Costruzione di un\u2019immagine Docker e conversione in SIF mostra come costruire e convertire immagini Docker in <code>.sif</code>.</p> <p>In Appendice, la sezione Dockerfile di esempio per ambiente Geant4/ROOT contiene un Dockerfile di esempio.</p> <p>Per comodit\u00e0, la sezione Immagini Docker / Apptainer disponibili su ReCaS riporta un elenco aggiornato delle immagini attualmente disponibili e dei relativi percorso reali su lustre, che possono essere riutilizzate come base per nuovi workflow.</p>"},{"location":"apptainer_condor/#sec-concetti-base","title":"Concetti di base: container, Apptainer e HTCondor","text":"<p>Prima di entrare negli esempi conviene chiarire cosa si intende per container e come Apptainer interagisce con HTCondor nel contesto del cluster ReCaS.</p> <p>Un container \u00e8 un ambiente software isolato, definito da un\u2019immagine che contiene un sistema operativo minimale (ad esempio Ubuntu), le librerie e le applicazioni necessarie. Quando si avvia un container, il programma viene eseguito con quell\u2019ambiente software, indipendentemente dal sistema operativo del nodo fisico. Nel nostro caso un\u2019immagine <code>.sif</code> contiene Geant4, ROOT, Python e le relative dipendenze, cos\u00ec che un job Condor non deve installare o configurare nulla: trova tutto gi\u00e0 predisposto.</p> <p>Su ReCaS i container sono gestiti da Apptainer (discendente di Singularity), progettato per ambienti HPC multiutente. Un\u2019immagine Apptainer \u00e8 un file in sola lettura; durante il job, Apptainer monta il filesystem dell\u2019immagine e allo stesso tempo monta la directory di lavoro dell\u2019utente, in modo che il programma possa leggere e scrivere i propri file su lustre. Questo approccio \u00e8 pi\u00f9 leggero di una macchina virtuale, perch\u00e9 il kernel del sistema \u00e8 condiviso e si avvia solo lo strato utente.</p> <p>HTCondor si occupa di individuare i worker node disponibili, preparare la directory di lavoro e avviare Apptainer. Dal punto di vista dell\u2019utente, la cosa fondamentale \u00e8 capire il ruolo di tre parametri nel file di submit: <code>initialdir</code>, <code>executable</code> e <code>container_image</code>.</p> <ul> <li>La direttiva <code>initialdir</code> indica la directory sul filesystem di lustre che rappresenta la cartella di lavoro del job. Condor monta questa directory nel container come current working directory (CWD), quindi tutto ci\u00f2 che viene scritto in CWD o in sottocartelle relative finisce direttamente in questa directory su lustre.</li> <li>La direttiva <code>executable</code> indica lo script o l\u2019eseguibile che verr\u00e0 lanciato all\u2019interno del container. Deve trovarsi nella <code>initialdir</code> o in una sua sottocartella ed \u00e8 specificato nel file di submit con un percorso relativo.</li> <li>La direttiva <code>container_image</code> indica quale immagine Apptainer usare. I test effettuati sul cluster hanno mostrato un comportamento pratico importante: Condor si aspetta che il valore di <code>container_image</code> sia il nome di un file presente nella <code>initialdir</code>. Per questa ragione, anche se l\u2019immagine \u201creale\u201d vive in una directory centrale, per ogni job conviene creare nella <code>initialdir</code> un symlink locale all\u2019immagine e poi usare nel submit il nome del symlink (ad esempio un symlink a <code>/lustrehome/alice/apptainer_images/immagine.sif</code>).</li> </ul> <p>In pratica, quando Condor avvia Apptainer, la <code>initialdir</code> viene montata nel container come directory di lavoro corrente: non c\u2019\u00e8 copia dei file, tutto ci\u00f2 che il job legge o scrive in CWD (e nelle sottocartelle relative) finisce direttamente sulla stessa directory di lustre.</p> <p> Attenzione: comportamento specifico di ReCaS</p> <p>Il requisito che <code>container_image</code> debba essere il nome di un file presente nella <code>initialdir</code> (tipicamente un symlink verso la <code>.sif</code> reale su lustre) \u00e8 legato alla configurazione di HTCondor su ReCaS. Al momento questa \u00e8 la modalit\u00e0 supportata e testata; in futuro potrebbero essere aggiunti anche path assoluti o altri meccanismi per individuare l\u2019immagine del container.</p> <p>Cosa fa davvero HTCondor con <code>container_image</code></p> <p>Quando nel file di submit si imposta:</p> Text Only<pre><code>container_image = G4_v11.3.1.sif\n</code></pre> <p>dal punto di vista pratico HTCondor, sul worker node, fa qualcosa di equivalente a:</p> <p>Bash<pre><code>apptainer exec G4_v11.3.1.sif ./script_che_hai_messo_in_executable.sh\n</code></pre> (oppure <code>singularity exec</code> a seconda del runtime disponibile).</p> <p>Questo significa che:</p> <ul> <li>l\u2019ambiente dentro il container \u00e8 lo stesso che avresti lanciando   <code>apptainer exec</code> a mano da shell;</li> <li>le variabili d\u2019ambiente e i PATH di Geant4/ROOT non vengono magicamente   settati da Condor, ma solo da eventuali script di entrypoint dell\u2019immagine oppure da quello che fai tu nello <code>executable</code> (ad es.):     Bash<pre><code>source /opt/geant4/bin/geant4.sh\nsource /opt/root/bin/thisroot.sh\n</code></pre></li> </ul> <p>Per questo, in tutti gli esempi della guida, gli script <code>*.sh</code> eseguiti come <code>executable</code> fanno esplicitamente il <code>source</code> degli script di ambiente di Geant4 e ROOT all\u2019inizio.</p> <p>Un esempio tipico \u00e8 il seguente. Nella directory del job di <code>bob</code> si crea un link all\u2019immagine condivisa di <code>alice</code>:</p> Bash<pre><code>ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif G4_v11.3.1.sif\n</code></pre> <p>e nel file di submit si scrive:</p> Bash<pre><code>container_image = G4_v11.3.1.sif\n</code></pre> <p>In questo modo Condor trova il file <code>G4_v11.3.1.sif</code> nella <code>initialdir</code>, avvia Apptainer con quell\u2019immagine e monta la <code>initialdir</code> all\u2019interno del container. Da quel momento in poi lo script <code>executable</code> viene eseguito dentro il container e la directory di lavoro corrisponde alla directory dell\u2019utente su lustre.</p> <p>Gli esempi pratici della sezione Esempi non fanno altro che declinare questo schema base in casi d\u2019uso via via pi\u00f9 complessi.</p>"},{"location":"apptainer_condor/#sec-organizzazione","title":"Organizzazione delle directory","text":"<p>Per lavorare in modo ordinato conviene scegliere una convenzione semplice all\u2019interno della propria home su lustre. Nel caso di <code>bob</code>, la directory di riferimento per i job \u00e8 <code>/lustrehome/bob</code>, mentre l\u2019utente manutentore <code>alice</code> usa <code>/lustrehome/alice</code> per ospitare le immagini condivise.</p> <p>Le immagini Apptainer condivise dal manutentore possono essere raccolte in una directory dedicata, ad esempio <code>/lustrehome/alice/apptainer_images</code>. In questa directory si collocano i file <code>.sif</code> che <code>alice</code> ha costruito o recuperato. Nel nostro esempio vi si trovano <code>G4_v11.3.1.sif</code> e <code>G4_v10.6.3_NOMULTITHREAD.sif</code>.</p> <p>Per gli esempi e i job Condor di <code>bob</code> si pu\u00f2 usare una directory <code>condor_tests</code>. All\u2019interno di <code>condor_tests</code> \u00e8 utile creare sottodirectory dedicate per ciascun tipo di job. Ogni directory contiene i file di submit <code>.csi</code>, gli script <code>.sh</code>, un link locale all\u2019immagine <code>.sif</code> (proveniente da <code>/lustrehome/alice/apptainer_images</code>) e una sottocartella <code>logs/</code> per gli output di Condor. Questa struttura rende chiaro dove si trova il codice sorgente, dove viene compilato il programma e dove finiscono i file prodotti dai job Condor, seguendo lo schema introdotto in Concetti di base: container, Apptainer e HTCondor e utilizzato in tutti gli esempi successivi.</p> <p> Esempio di gerarchia tipica su lustre</p> <p>Una possibile organizzazione delle directory per <code>alice</code> (manutentore) e <code>bob</code> (utente che sottomette i job) pu\u00f2 essere:</p> Text Only<pre><code>/lustrehome/alice/\n  apptainer_images/\n    G4_v11.3.1.sif\n    G4_v10.6.3_NOMULTITHREAD.sif\n\n/lustrehome/bob/\n  condor_tests/\n    test_container/\n      logs/\n      G4_v11.3.1.sif -&gt; /lustrehome/alice/apptainer_images/G4_v11.3.1.sif\n      test_container.sh\n      test_container.csi\n\n    build_B5_11.3.1/\n      logs/\n      G4_v11.3.1.sif -&gt; /lustrehome/alice/apptainer_images/G4_v11.3.1.sif\n      build_B5_exec.sh\n      build_B5_11.3.1.csi\n      B5_build_condor/      # directory di build creata dal job\n\n    run_B5_11.3.1/\n      logs/\n      G4_v11.3.1.sif -&gt; /lustrehome/alice/apptainer_images/G4_v11.3.1.sif\n      run_B5_exec.sh\n      run_B5_11.3.1.csi\n      exampleB5\n      run1.mac\n      # file di output prodotti da B5\n</code></pre> <p>In questa configurazione:</p> <ul> <li><code>alice</code> mantiene tutte le immagini <code>.sif</code> in un\u2019unica directory centrale;</li> <li><code>bob</code> crea, per ogni tipo di job, una directory dedicata con i file <code>.csi</code>,   gli script <code>.sh</code>, una sottocartella <code>logs/</code> e un symlink locale all\u2019immagine <code>.sif</code>.</li> </ul>"},{"location":"apptainer_condor/#sec-esempi","title":"Esempi","text":"<p>In questa sezione sono riportati cinque esempi completi che illustrano come utilizzare immagini Apptainer/Singularity in combinazione con HTCondor. Gli esempi seguono un ordine progressivo, dal test pi\u00f9 semplice fino a un caso realistico con un progetto Geant4 personalizzato:</p> <ul> <li>Esempio 1 \u2013 test minimale dell\u2019immagine <code>.sif</code> per verificare la versione di Geant4, ROOT e Python e controllare che il container venga avviato correttamente su un worker node;</li> <li>Esempio 2 \u2013 compilazione dell\u2019esempio Geant4 B5 all\u2019interno del container utilizzando CMake;</li> <li>Esempio 3 \u2013 esecuzione di un binario Geant4 precompilato con una macro, in un job dedicato;</li> <li>Esempio 4 \u2013 compilazione ed esecuzione dell\u2019esempio B5 nello stesso job HTCondor, utile quando si vuole una build \u201cpulita\u201d per ogni run;</li> <li>Esempio 5 \u2013 caso realistico con il progetto CsI-WLS, che prevede build con CMake e un batch di simulazioni pilotato da uno script Python.</li> </ul> <p>I template completi degli script <code>.sh</code> e dei file di submit <code>.csi</code> utilizzati negli esempi successivi sono disponibili nella cartella <code>templates/</code> del repository GitHub.</p> <p>In particolare:</p> <ul> <li><code>Es1_test_container</code></li> <li><code>Es2_build_geant_project</code></li> <li><code>Es3_run_geant_exec</code></li> <li><code>Es4_build_and_run_geant_example</code></li> <li><code>Es5_build_and_run_python</code></li> </ul>"},{"location":"apptainer_condor/#sec-esempio1","title":"Esempio 1: test dell'immagine Geant4","text":"<p>Il primo esempio ha lo scopo di verificare che l\u2019immagine <code>G4_v11.3.1.sif</code> funzioni correttamente su un worker node. L\u2019obiettivo \u00e8 sapere su quale nodo gira il job, quali versioni di Geant4, ROOT e Python sono visibili dall\u2019interno del container e se l\u2019ambiente \u00e8 coerente.</p> <p>Si inizia creando la directory del test e una sottocartella per i log:</p> Bash<pre><code>cd /lustrehome/bob\nmkdir -p condor_tests/test_container/logs\ncd condor_tests/test_container\n</code></pre> <p>Si crea un link all\u2019immagine condivisa di <code>alice</code>:</p> Bash<pre><code>ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\\n       G4_v11.3.1.sif\n</code></pre> <p>Lo script <code>test_container.sh</code>, che sar\u00e0 eseguito dentro il container, pu\u00f2 essere definito come segue:</p> Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[TEST] Start: $(date)\"\necho \"[TEST] Host:  $(hostname)\"\necho \"[TEST] User:  $(whoami)\"\necho \"[TEST] Pwd:   $(pwd)\"\necho\n\necho \"[TEST] Environment snippet:\"\necho \"  G4INSTALL=${G4INSTALL:-undefined}\"\necho \"  G4VERSION=${G4VERSION:-undefined}\"\necho \"  ROOTSYS=${ROOTSYS:-undefined}\"\necho\n\necho \"[TEST] Checking Geant4 / ROOT / Python...\"\ncommand -v geant4-config  &gt;/dev/null 2&gt;&amp;1 &amp;&amp; \\\n  geant4-config --version || echo \"geant4-config NOT found\"\ncommand -v root-config    &gt;/dev/null 2&gt;&amp;1 &amp;&amp; \\\n  root-config --version   || echo \"root-config NOT found\"\ncommand -v python3        &gt;/dev/null 2&gt;&amp;1 &amp;&amp; \\\n  python3 --version       || echo \"python3 NOT found\"\necho\n\npython3 - &lt;&lt; 'EOF'\nimport sys, platform\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Platform:\", platform.platform())\nEOF\n\necho\necho \"[TEST] Done: $(date)\"\n</code></pre> <p>Lo script va reso eseguibile:</p> Bash<pre><code>chmod +x test_container.sh\n</code></pre> <p>Il file di submit <code>test_container.csi</code> specifica la directory iniziale, lo script da eseguire e l\u2019immagine da usare:</p> Bash<pre><code>universe        = vanilla\n\ninitialdir      = /lustrehome/bob/condor_tests/test_container\n\nexecutable      = test_container.sh\narguments       =\n\ncontainer_image = G4_v11.3.1.sif\n\nrequest_cpus    = 1\nrequest_memory  = 1 GB\nrequest_disk    = 4 GB\n\noutput          = logs/test_$(ClusterId).$(ProcId).out\nerror           = logs/test_$(ClusterId).$(ProcId).err\nlog             = logs/test_$(ClusterId).$(ProcId).log\n\nqueue 1\n</code></pre> <p>La sottomissione avviene con:</p> Bash<pre><code>condor_submit test_container.csi\n</code></pre> <p>Quando il job \u00e8 completato, il file <code>logs/test_...out</code> contiene le informazioni stampate dallo script: il nome del worker node, l\u2019utente, la directory di lavoro, le variabili di ambiente e le versioni dei software principali. Questo conferma che l\u2019immagine \u00e8 correttamente utilizzabile con HTCondor secondo lo schema introdotto in Concetti di base: container, Apptainer e HTCondor.</p> <p> Job in HOLD: controlli rapidi</p> <p>Se il job va in stato HOLD subito dopo la sottomissione, controllare:</p> <ul> <li> <p>Symlink alla <code>.sif</code>   Verificare che il link non sia rotto:   Bash<pre><code>ls -l G4_v11.3.1.sif\n</code></pre>   Deve puntare a <code>/lustrehome/alice/apptainer_images/G4_v11.3.1.sif</code> (o percorso equivalente).</p> </li> <li> <p>Esistenza e permessi dell\u2019immagine reale   Controllare che il file reale esista e sia leggibile:   Bash<pre><code>ls -l /lustrehome/alice/apptainer_images/G4_v11.3.1.sif\n</code></pre></p> </li> <li> <p>Coerenza del nome in <code>container_image</code>   Nel file di submit, il valore di:   Text Only<pre><code>container_image = G4_v11.3.1.sif\n</code></pre>   deve coincidere esattamente con il nome del file presente nella <code>initialdir</code>.</p> </li> <li> <p>Percorso corretto di <code>initialdir</code>   Verificare che la directory indicata da <code>initialdir</code> esista e contenga sia   lo script <code>executable</code> sia il symlink all\u2019immagine.</p> </li> </ul>"},{"location":"apptainer_condor/#sec-esempio2","title":"Esempio 2: build dell\u2019esempio B5 di Geant4","text":"<p>Il secondo esempio mostra come compilare l\u2019esempio B5 di Geant4 usando l\u2019immagine <code>G4_v11.3.1.sif</code>. L\u2019obiettivo \u00e8 ottenere l\u2019eseguibile <code>exampleB5</code> in una directory di build gestita dal job.</p> <p>Si crea la directory del test di build:</p> Bash<pre><code>cd /lustrehome/bob\nmkdir -p condor_tests/build_B5_11.3.1/logs\ncd condor_tests/build_B5_11.3.1\n</code></pre> <p>Si collega l\u2019immagine condivisa:</p> Bash<pre><code>ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif G4_v11.3.1.sif\n</code></pre> <p>Lo script <code>build_B5_exec.sh</code> viene eseguito dentro il container e si occupa di configurare e compilare il progetto:</p> Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[BUILD] Start: $(date)\"\necho \"[BUILD] Host:  $(hostname)\"\necho \"[BUILD] User:  $(whoami)\"\necho \"[BUILD] Pwd:   $(pwd)\"\necho\n\nSRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\"\nBUILD_DIR=\"B5_build_condor\"\n\necho \"[BUILD] SRC_DIR   = ${SRC_DIR}\"\necho \"[BUILD] BUILD_DIR = ${BUILD_DIR}\"\necho\n\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[BUILD] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\n\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[BUILD] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nmkdir -p \"${BUILD_DIR}\"\ncd \"${BUILD_DIR}\"\n\necho \"[BUILD] Now in: $(pwd)\"\n\necho \"[BUILD] Running CMake...\"\ncmake \"${SRC_DIR}\"\n\necho \"[BUILD] Building...\"\ncmake --build . -- -j\"$(nproc)\"\n\necho\necho \"[BUILD] Done: $(date)\"\n</code></pre> <p>Lo script viene reso eseguibile:</p> Bash<pre><code>chmod +x build_B5_exec.sh\n</code></pre> <p>Il file di submit <code>build_B5_11.3.1.csi</code> \u00e8:</p> Bash<pre><code>universe        = vanilla\n\ninitialdir      = /lustrehome/bob/condor_tests/build_B5_11.3.1\n\nexecutable      = build_B5_exec.sh\narguments       =\n\ncontainer_image = G4_v11.3.1.sif\n\nrequest_cpus    = 4\nrequest_memory  = 4 GB\nrequest_disk    = 8 GB\n\noutput          = logs/build_$(ClusterId).$(ProcId).out\nerror           = logs/build_$(ClusterId).$(ProcId).err\nlog             = logs/build_$(ClusterId).$(ProcId).log\n\nqueue 1\n</code></pre> <p>Il job si sottomette con:</p> Bash<pre><code>condor_submit build_B5_11.3.1.csi\n</code></pre> <p>Al termine della compilazione, nella directory <code>/lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor</code> si trovano i file di CMake e l\u2019eseguibile <code>exampleB5</code>. Lo standard output del job contiene i messaggi di CMake e l\u2019esito del build, che verr\u00e0 riutilizzato nella sezione Esempio 3: run dell\u2019esempio B5.</p>"},{"location":"apptainer_condor/#sec-esempio3","title":"Esempio 3: run dell\u2019esempio B5","text":"<p>Il terzo esempio riutilizza l\u2019eseguibile <code>exampleB5</code> compilato in Esempio 2: build dell\u2019esempio B5 di Geant4 e mostra come preparare una directory di run separata, in cui copiare l\u2019eseguibile e le macro e lanciare la simulazione.</p> <p>Si crea la directory per il run:</p> Bash<pre><code>cd /lustrehome/bob\nmkdir -p condor_tests/run_B5_11.3.1/logs\ncd condor_tests/run_B5_11.3.1\n</code></pre> <p>Si collega l\u2019immagine:</p> Bash<pre><code>ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif G4_v11.3.1.sif\n</code></pre> <p>Si copiano l\u2019eseguibile e la macro <code>run1.mac</code> dalla build:</p> Bash<pre><code>cp /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor/exampleB5 .\ncp /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor/run1.mac .\n</code></pre> <p>Lo script <code>run_B5_exec.sh</code> lancia l\u2019eseguibile con la macro:</p> Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\nif [[ $# -lt 2 ]]; then\n  echo \"Usage: $0 &lt;exec_rel_path&gt; &lt;macro_rel_path&gt;\"\n  echo \"Example: $0 exampleB5 run1.mac\"\n  exit 1\nfi\n\nEXEC_REL=\"$1\"\nMACRO_REL=\"$2\"\n\necho \"[RUN] Start: $(date)\"\necho \"[RUN] Host:  $(hostname)\"\necho \"[RUN] User:  $(whoami)\"\necho \"[RUN] Pwd:   $(pwd)\"\necho \"[RUN] Exec:  ${EXEC_REL}\"\necho \"[RUN] Macro: ${MACRO_REL}\"\necho\n\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[RUN] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[RUN] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nEXEC_PATH=\"./${EXEC_REL}\"\nMACRO_PATH=\"./${MACRO_REL}\"\n\nif [[ ! -x \"${EXEC_PATH}\" ]]; then\n  echo \"[RUN] ERROR: executable not found or not executable: ${EXEC_PATH}\"\n  exit 1\nfi\nif [[ ! -f \"${MACRO_PATH}\" ]]; then\n  echo \"[RUN] ERROR: macro not found: ${MACRO_PATH}\"\n  exit 1\nfi\n\necho \"[RUN] Launching: ${EXEC_PATH} ${MACRO_PATH}\"\n\"${EXEC_PATH}\" \"${MACRO_PATH}\"\n\necho\necho \"[RUN] Done: $(date)\"\n</code></pre> <p>Lo script va reso eseguibile:</p> Bash<pre><code>chmod +x run_B5_exec.sh\n</code></pre> <p>Il file di submit <code>run_B5_11.3.1.csi</code> utilizza lo script, l\u2019immagine e specifica le risorse richieste:</p> Bash<pre><code>universe        = vanilla\n\ninitialdir      = /lustrehome/bob/condor_tests/run_B5_11.3.1\n\nexecutable      = run_B5_exec.sh\narguments       = exampleB5 run1.mac\n\ncontainer_image = G4_v11.3.1.sif\n\nrequest_cpus    = 1\nrequest_memory  = 2 GB\nrequest_disk    = 4 GB\n\noutput          = logs/run_$(ClusterId).$(ProcId).out\nerror           = logs/run_$(ClusterId).$(ProcId).err\nlog             = logs/run_$(ClusterId).$(ProcId).log\n\nqueue 1\n</code></pre> <p>La sottomissione avviene come nei casi precedenti:</p> Bash<pre><code>condor_submit run_B5_11.3.1.csi\n</code></pre> <p>Gli output prodotti da B5 vengono scritti nella directory <code>/lustrehome/bob/condor_tests/run_B5_11.3.1</code>, che corrisponde alla directory di lavoro del job dentro il container, e rimangono quindi disponibili all\u2019utente anche dopo la fine dell\u2019esecuzione.</p>"},{"location":"apptainer_condor/#sec-esempio4","title":"Esempio 4: build e run di B5 in un unico job","text":"<p>In alcune situazioni \u00e8 comodo compilare il codice e far partire subito il run all\u2019interno dello stesso job Condor. Questo permette di avere un singolo file di submit per l\u2019intera catena e di garantire che il run utilizzi esattamente la build prodotta nel job.</p> <p>Per questo esempio si crea una nuova directory:</p> Bash<pre><code>cd /lustrehome/bob\nmkdir -p condor_tests/build_run_B5_11.3.1/logs\ncd condor_tests/build_run_B5_11.3.1\n</code></pre> <p>Si collega l\u2019immagine Geant4 11.3.1:</p> Bash<pre><code>ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif G4_v11.3.1.sif\n</code></pre> <p>Lo script <code>build_run_B5_exec.sh</code> esegue prima la compilazione dell\u2019esempio B5 e subito dopo l\u2019eseguibile con una macro di esempio:</p> Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[BUILD+RUN] Start: $(date)\"\necho \"[BUILD+RUN] Host:  $(hostname)\"\necho \"[BUILD+RUN] User:  $(whoami)\"\necho \"[BUILD+RUN] Pwd:   $(pwd)\"\necho\n\nSRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\"\nBUILD_DIR=\"B5_build_condor\"\nMACRO=\"${SRC_DIR}/run1.mac\"\n\necho \"[BUILD+RUN] SRC_DIR   = ${SRC_DIR}\"\necho \"[BUILD+RUN] BUILD_DIR = ${BUILD_DIR}\"\necho \"[BUILD+RUN] MACRO     = ${MACRO}\"\necho\n\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[BUILD+RUN] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\n\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[BUILD+RUN] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nmkdir -p \"${BUILD_DIR}\"\ncd \"${BUILD_DIR}\"\n\necho \"[BUILD+RUN] Now in: $(pwd)\"\n\nif [[ -x exampleB5 ]]; then\n  echo \"[BUILD+RUN] exampleB5 already built, skipping CMake/cmake --build.\"\nelse\n  echo \"[BUILD+RUN] Running CMake...\"\n  cmake \"${SRC_DIR}\"\n\n  echo \"[BUILD+RUN] Building...\"\n  cmake --build . -- -j\"$(nproc)\"\nfi\n\nif [[ ! -x exampleB5 ]]; then\n  echo \"[BUILD+RUN] ERROR: build failed, exampleB5 not found.\"\n  exit 1\nfi\n\necho\necho \"[BUILD+RUN] Running exampleB5 with macro: ${MACRO}\"\n./exampleB5 \"${MACRO}\"\n\necho\necho \"[BUILD+RUN] ROOT files under $(pwd):\"\nfind . -maxdepth 3 -type f -name '*.root' -print || \\\n  echo \"[BUILD+RUN] Nessun .root trovato.\"\n\necho\necho \"[BUILD+RUN] Done: $(date)\"\n</code></pre> <p>Lo script viene reso eseguibile:</p> Bash<pre><code>chmod +x build_run_B5_exec.sh\n</code></pre> <p>Il file di submit <code>build_run_B5_11.3.1.csi</code> \u00e8 il seguente:</p> Bash<pre><code>universe        = vanilla\n\ninitialdir      = /lustrehome/bob/condor_tests/build_run_B5_11.3.1\n\nexecutable      = build_run_B5_exec.sh\narguments       =\n\ncontainer_image = G4_v11.3.1.sif\n\nrequest_cpus    = 4\nrequest_memory  = 4 GB\nrequest_disk    = 8 GB\n\noutput          = logs/build_run_$(ClusterId).$(ProcId).out\nerror           = logs/build_run_$(ClusterId).$(ProcId).err\nlog             = logs/build_run_$(ClusterId).$(ProcId).log\n\nqueue 1\n</code></pre> <p>La sottomissione avviene con:</p> Bash<pre><code>condor_submit build_run_B5_11.3.1.csi\n</code></pre> <p>Alla conclusione del job, la directory <code>B5_build_condor</code> contiene sia i file generati da CMake, sia l\u2019eseguibile <code>exampleB5</code>, sia i file di output del run (ad esempio file ROOT). Tutti questi file risiedono in <code>/lustrehome/bob/condor_tests/build_run_B5_11.3.1/B5_build_condor</code> e sono quindi accessibili per analisi successive.</p>"},{"location":"apptainer_condor/#sec-esempio5","title":"Esempio 5: progetto CsI-WLS con Python","text":"<p>L\u2019ultimo esempio mostra una situazione pi\u00f9 vicina a un caso reale, in cui un\u2019applicazione Geant4 custom (CsI-WLS) viene compilata da sorgente con CMake e poi eseguita molte volte con parametri diversi, gestiti da uno script Python. Per questo caso si sfrutta l\u2019immagine <code>G4_v10.6.3_NOMULTITHREAD.sif</code>, che contiene Geant4 10.6.3 senza multithreading, ROOT 6.36.4 e Python3 con le librerie principali.</p> <p>Si assume che il progetto CsI-WLS esista gi\u00e0 in una directory di sviluppo, ad esempio <code>/lustrehome/bob/ADAPT/simulation/CsI-WLS_v1.2.2</code>, che contiene una sottodirectory <code>src</code> con i file CMake e il codice. Per rendere il job auto-contenuto dentro <code>condor_tests</code> si copia solo la directory <code>src</code>:</p> Bash<pre><code>cd /lustrehome/bob/condor_tests\nmkdir -p CsI-WLS_v1.2.2\ncp -r /lustrehome/bob/ADAPT/simulation/CsI-WLS_v1.2.2/src CsI-WLS_v1.2.2/\ncd CsI-WLS_v1.2.2\nmkdir -p logs\n</code></pre> <p>Si crea il link all\u2019immagine senza multithreading di <code>alice</code>:</p> Bash<pre><code>ln -sf /lustrehome/alice/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif G4_v10.6.3_NOMULTITHREAD.sif\n</code></pre> <p>Nella directory <code>src</code> si definisce lo script Python che si aspetta di essere eseguito da dentro <code>build</code> (dove esiste <code>./CsI-WLS</code>). Questo script genera un certo numero di macro, ognuna con un seed diverso, e per ciascuna macro lancia l\u2019eseguibile:</p> Python<pre><code>import numpy as np\nimport os\nimport matplotlib.pyplot as plt \n\nDIR_MAC = \"./mac_electron\"\nDIR_ROOT = \"rootOutput_electron\"\n\nN_EVENTS = 10\nsubdir = \"random_rectangular_source_200keV\"\nnfiles = 50\n\nfor k in range(nfiles):\n    seed1, seed2 = np.random.randint(0, 2**32, size=2)\n\n    mac = (f\"{DIR_MAC}/{subdir}/\"\n           f\"random_rectangular_source_electron_ene_\"\n           f\"200keV_ly1_n{k}.mac\")\n    root = mac.replace(DIR_MAC, DIR_ROOT).replace(\".mac\", \"\")\n\n    os.makedirs(os.path.dirname(mac),  exist_ok=True)\n    os.makedirs(os.path.dirname(root), exist_ok=True)\n\n    with open(mac, \"w\") as f:\n        f.write(\n            \"/run/initialize\\n\"\n            \"/tracking/verbose 0\\n\"\n            \"/gps/particle e-\\n\"\n            \"/gps/position 0 0 0 mm\\n\"\n            \"/gps/direction 0 0 -1\\n\"\n            f\"/random/setSeeds [{seed1} {seed2}]\\n\"\n            \"/gps/pos/type Plane\\n\"\n            \"/gps/pos/shape Rectangle\\n\"\n            \"/gps/pos/halfx 220 mm\\n\"\n            \"/gps/pos/halfy 220 mm\\n\"\n            \"/gps/ene/mono 200 keV\\n\"\n            f\"/RunManager/NameOfOutputFile {root}\\n\"\n            f\"/run/beamOn {N_EVENTS}\\n\"\n        )\n\n    os.system(f\"./CsI-WLS {mac}\")\n\nprint(f\"\\n {N_EVENTS*nfiles} eventi salvati in \"\n      f\"{DIR_MAC}/{subdir} e simulati con ./CsI-WLS\")\n</code></pre> <p>Questo file pu\u00f2 essere salvato come <code>src/run_electrons_batch.py</code> all\u2019interno della copia di CsI-WLS sotto <code>condor_tests</code>.</p> <p>Nella root della directory <code>CsI-WLS_v1.2.2</code> si definisce lo script <code>run_CsI_WLS_electron_batch.sh</code>, che compila il progetto nella directory <code>build</code> e poi lancia lo script Python:</p> Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[PYRUN] Start: $(date)\"\necho \"[PYRUN] Host: $(hostname)\"\necho \"[PYRUN] User: $(whoami)\"\necho \"[PYRUN] Pwd:  $(pwd)\"\necho\n\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[PYRUN] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[PYRUN] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nBUILD_DIR=\"build\"\nSRC_DIR=\"src\"\n\nmkdir -p \"${BUILD_DIR}\"\ncd \"${BUILD_DIR}\"\n\necho \"[PYRUN] Now in build dir: $(pwd)\"\n\nif [[ -x CsI-WLS ]]; then\n  echo \"[PYRUN] CsI-WLS already built, skipping cmake/cmake --build.\"\nelse\n  echo \"[PYRUN] Configuring with CMake...\"\n  cmake \"../${SRC_DIR}\"\n\n  echo \"[PYRUN] Building CsI-WLS...\"\n  cmake --build . -- -j\"$(nproc)\"\nfi\n\nif [[ ! -x CsI-WLS ]]; then\n  echo \"[PYRUN] ERROR: CsI-WLS binary not found after build.\"\n  exit 1\nfi\n\necho\necho \"[PYRUN] Running Python batch script...\"\npython3 ../source/run_electrons_batch.py\n\necho\necho \"[PYRUN] ROOT files under rootOutput_electron/:\"\nfind rootOutput_electron -maxdepth 3 -type f -name '*.root' -print \\\n  || echo \"[PYRUN] Nessun .root trovato.\"\n\necho\necho \"[PYRUN] Done at $(date)\"\n</code></pre> <p>Lo script va reso eseguibile:</p> Bash<pre><code>chmod +x run_CsI_WLS_electron_batch.sh\n</code></pre> <p>Infine si definisce il file di submit <code>CsI_WLS_python_electrons.csi</code>:</p> Bash<pre><code>universe        = vanilla\n\ninitialdir      = /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2\n\nexecutable      = run_CsI_WLS_electron_batch.sh\narguments       =\n\ncontainer_image = G4_v10.6.3_NOMULTITHREAD.sif\n\nrequest_cpus    = 1\nrequest_memory  = 2 GB\nrequest_disk    = 16 GB\n\noutput          = logs/pyCsI_$(ClusterId).$(ProcId).out\nerror           = logs/pyCsI_$(ClusterId).$(ProcId).err\nlog             = logs/pyCsI_$(ClusterId).$(ProcId).log\n\nqueue 1\n</code></pre> <p>La sottomissione avviene con:</p> Bash<pre><code>cd /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2\ncondor_submit CsI_WLS_python_electrons.csi\n</code></pre> <p>Quando il job \u00e8 terminato, nella directory <code>build</code> compaiono l\u2019eseguibile <code>CsI-WLS</code>, le macro generate dallo script Python e gli output ROOT. Tutti questi file sono salvati su lustre dentro <code>/lustrehome/bob/condor_tests/CsI-WLS_v1.2.2</code>.</p>"},{"location":"apptainer_condor/#sec-docker-sif","title":"Costruzione di un\u2019immagine Docker e conversione in SIF","text":"<p>Gli esempi della sezione Esempi assumono che le immagini <code>.sif</code> siano gi\u00e0 disponibili in una cartella condivisa, gestita dall\u2019utente <code>alice</code>. Questa sezione descrive in modo sintetico come costruire un\u2019immagine Docker con Geant4, ROOT e Python, come convertirla in un\u2019immagine Apptainer/Singularity e come distribuirla agli altri utenti, senza entrare nei dettagli dei singoli comandi di installazione del software all\u2019interno del container.</p> <p>La costruzione di immagini Docker richiede una macchina con Docker installato e accessibile, ad esempio le macchine come <code>tesla02.recas.infn.ba.it</code>. Un utente pu\u00f2 connettersi a quella macchina con le stesse credenziali delle macchine di frontend <code>ui-al9.recas.infn.ba.it</code>, preparare un <code>Dockerfile</code> e costruire l\u2019immagine. Un esempio completo di <code>Dockerfile</code> per un ambiente Ubuntu 24.04 con Geant4, ROOT e Python3 \u00e8 riportato in appendice, nella sezione Dockerfile di esempio per ambiente Geant4/ROOT.</p> <p> Dove gira Docker su ReCaS</p> <p>Attualmente Docker \u00e8 installato solo sulla macchina <code>tesla02.recas.ba.infn.it</code>. Per usarlo occorre prima collegarsi da una delle macchine di frontend:</p> Bash<pre><code>ssh username@tesla02.recas.ba.infn.it\n</code></pre> <p>Al primo accesso viene creata automaticamente la home locale su <code>tesla02</code>. Tutta la fase di build e test interattivo delle immagini Docker va fatta su <code>tesla02</code>; le immagini poi vengono caricate sul registry <code>registry-clustergpu.recas.ba.infn.it</code> e da l\u00ec convertite in immagini Apptainer/Singularity utilizzabili con HTCondor sui worker node.</p> <p>Una volta scritto il <code>Dockerfile</code>, l\u2019utente manutentore pu\u00f2 costruire l\u2019immagine con un comando del tipo:</p> Bash<pre><code>docker build -t registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 .\n</code></pre> <p>In seguito, l\u2019immagine pu\u00f2 essere inviata al registry interno, dove le credenziali sono le stesse del frontend:</p> Bash<pre><code>docker login registry-clustergpu.recas.ba.infn.it\ndocker push registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3\n</code></pre> <p>La conversione da immagine Docker a immagine Apptainer/Singularity deve avvenire un worker node dove Apptainer \u00e8 installato e eseguire il comando <code>build</code>. Un esempio di comando \u00e8:</p> Bash<pre><code>apptainer build G4_v10.6.3.sif docker://registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3\n</code></pre> <p>Il file <code>G4_v10.6.3.sif</code> cos\u00ec generato pu\u00f2 essere copiato o spostato nella directory condivisa delle immagini:</p> Bash<pre><code>mv G4_v10.6.3.sif /lustrehome/alice/apptainer_images/\n</code></pre> <p>Dopo questo passaggio, tutti gli utenti (come <code>bob</code>) possono usare l\u2019immagine nei propri job Condor creando un symlink nella <code>initialdir</code> e impostando <code>container_image</code> al nome del symlink, come mostrato nella sezione Concetti di base: container, Apptainer e HTCondor e negli esempi.</p> <p>Tip</p> <p>Sia la build della immagine Docker che la conversione in <code>.sif</code> pu\u00f2 anche essere fatta in locale sul proprio pc e poi copiata su una directory accessibile su lustrehome. </p>"},{"location":"apptainer_condor/#comandi-essenziali-docker","title":"Comandi essenziali Docker","text":"<p>Non \u00e8 necessario che ogni utente conosca Docker in dettaglio, ma \u00e8 utile riassumere i comandi pi\u00f9 usati nel ciclo di vita di un\u2019immagine. La costruzione da <code>Dockerfile</code> nella directory corrente avviene di solito con:</p> Bash<pre><code>docker build -t nome_immagine:tag .\n</code></pre> <p>Per visualizzare le immagini locali si pu\u00f2 usare:</p> Bash<pre><code>docker images\n</code></pre> <p>Per testare interattivamente un\u2019immagine, ad esempio verificando che gli script di ambiente siano corretti, si pu\u00f2 avviare un container con:</p> Bash<pre><code>docker run -it nome_immagine:tag\n</code></pre> <p>Infine, per taggare e inviare un\u2019immagine verso il registry remoto, si possono usare comandi del tipo:</p> Bash<pre><code>docker tag nome_immagine:tag registry-clustergpu.recas.ba.infn.it/alice/nome_immagine:tag\n\ndocker push registry-clustergpu.recas.ba.infn.it/alice/nome_immagine:tag\n</code></pre> <p>Questi comandi vengono eseguiti sulla macchina che ha Docker installato, come <code>tesla02.recas.infn.ba.it</code>.</p>"},{"location":"apptainer_condor/#comandi-essenziali-apptainersingularity","title":"Comandi essenziali Apptainer/Singularity","text":"<p>Apptainer viene utilizzato sia per testare manualmente le immagini sia in maniera indiretta tramite HTCondor. Per un test rapido si pu\u00f2 eseguire un comando dentro un\u2019immagine <code>.sif</code> con:</p> Bash<pre><code>apptainer exec G4_v11.3.1.sif geant4-config --version\n</code></pre> <p>Per aprire una shell interattiva nel container si pu\u00f2 usare:</p> Bash<pre><code>apptainer shell G4_v11.3.1.sif\n</code></pre> <p>La conversione da immagine Docker a <code>.sif</code> avviene, come gi\u00e0 mostrato, con:</p> Bash<pre><code>apptainer build G4_v11.3.1.sif \\\n  docker://registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n</code></pre> <p>I job Condor che usano <code>container_image</code> nascondono questi dettagli, perch\u00e9 \u00e8 il sistema a chiamare internamente Apptainer. Tuttavia, conoscere questi comandi aiuta a testare rapidamente un\u2019immagine su una macchina di frontend prima di costruire i file <code>.csi</code>, come quelli della sezione Esempi.</p> <p> Dove gira Docker su ReCaS</p> <p>Per poter reindirizzare eventuali finestre grafiche dall'interno del container, ad esempio se si vuole visualizzare la geometria di un progetto Geant o il TBrowser grafico di ROOT, \u00e8 necessario settare la variabile d'ambiente <code>DISPLAY</code> tramite il seguente comando:</p> Bash<pre><code>apptainer run --cleanenv --env DISPLAY=$DISPLAY G4_v11.3.1.sif\n</code></pre>"},{"location":"apptainer_condor/#sec-images","title":"Immagini Docker / Apptainer disponibili su ReCaS","text":"<p>Questa sezione elenca alcune immagini gi\u00e0 presenti sul registry di ReCaS o su lustre che possono essere riutilizzate come base per nuovi progetti. L\u2019elenco non \u00e8 esaustivo e va considerato come \u201cfotografia\u201d dello stato attuale: nel tempo possono essere aggiunte nuove immagini o aggiornate le versioni.</p> # Docker image Singularity/Apptainer path Contenuto principale Note 1 <code>federicacuna/herd_centos07_devtoolset11_root:v2</code> \u2013 CentOS 7 + devtoolset-11 + ROOT, ambiente per HerdSoftware Immagine su Docker Hub/registry esterno. 2 <code>megalib-3.06</code> \u2013 MEGAlib 3.06 con tutte le dipendenze necessarie Usata per simulazioni MEGAlib. 3 <code>registry-clustergpu.recas.ba.infn.it/marcocecca/geant4:10.6.3</code> <code>/lustrehome/marcocecca/apptainer_images/G4_v10.6.3.sif</code> Ubuntu 24.04 + Geant4 10.6.3 + ROOT 6.36.4 + Python3/numpy/matplotlib Dati Geant4 inclusi; env Geant4/ROOT inizializzato all\u2019apertura del container. 4 <code>registry-clustergpu.recas.ba.infn.it/marcocecca/geant4:10.6.3_NOMULTITHREAD</code> <code>/lustrehome/marcocecca/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif</code> Come (3), ma con Geant4 10.6.3 compilato senza multithreading Adatta a job single-thread (uno per core) con HTCondor. 5 <code>registry-clustergpu.recas.ba.infn.it/marcocecca/geant4:11.3.1</code> <code>/lustrehome/marcocecca/apptainer_images/G4_v11.3.1.sif</code> Ubuntu 24.04 + Geant4 11.3.1 + ROOT 6.36.4 + Python3/numpy/matplotlib Dati Geant4 inclusi; env Geant4/ROOT inizializzato all\u2019apertura del container. <p>Se si utilizza una di queste immagini come base per nuovi workflow, \u00e8 buona pratica:</p> <ul> <li>documentare nel proprio progetto quale tag specifico si sta usando;</li> <li>generare nuove immagini con tag/versioni diverse quando servono modifiche significative;</li> <li>mantenere questa sezione aggiornata nel tempo, aggiungendo righe per nuove immagini \u201cufficiali\u201d.</li> </ul>"},{"location":"apptainer_condor/#appendice","title":"Appendice","text":""},{"location":"apptainer_condor/#sec-dockerfile-esempio","title":"Dockerfile di esempio per ambiente Geant4/ROOT","text":"<p>In questa sezione \u00e8 riportato un esempio completo di <code>Dockerfile</code> per costruire un\u2019immagine Docker basata su Ubuntu 24.04, con Geant4, ROOT e Python3. Il risultato atteso \u00e8 un\u2019immagine che espone gli script di environment <code>/opt/geant4/bin/geant4.sh</code> e <code>/opt/root/bin/thisroot.sh</code> e che pu\u00f2 essere convertita in un file <code>.sif</code> come descritto nella sezione Costruzione di un\u2019immagine Docker e conversione in SIF.</p> <p> USERNAME, USERID e GROUPID vanno modificati</p> <p>Nel Dockerfile di esempio, i campi: Docker<pre><code>ENV USERNAME=alice\nENV USERID=000001\nENV GROUPID=1234\n</code></pre> sono solo segnaposto. Prima di eseguire <code>docker build</code> vanno sostituiti con:</p> <ul> <li>il proprio nome utente su ReCaS (<code>USERNAME</code>),</li> <li>il proprio UID numerico (<code>USERID</code>),</li> <li>il proprio GID numerico (<code>GROUPID</code>).</li> </ul> <p>I valori corretti si ottengono, dalle macchine di frontend con: Bash<pre><code>id\n# uid=881525(alice) gid=2435(alice) groups=...\n</code></pre></p> <p>L\u2019uso di un utente reale all\u2019interno del container (con UID/GID coerenti a quelli del cluster) \u00e8 richiesto dai manuali ufficiali di ReCaS per garantire che i file scritti dal container abbiano permessi corretti su lustre.</p> Docker<pre><code>FROM ubuntu:24.04\n\nLABEL author=\"marcocecca\"\nLABEL version=\"G4v11.3.1_Rootv6.36.4_Ubuntu24\"\n\n# ===========================\n# Env Geant4 + ROOT (base)\n# ===========================\nENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC\n\nENV G4VERSION=11.3.1\nENV G4INSTALL=/opt/geant4\nENV G4DATA_DIR=$G4INSTALL/share/Geant4/data\nENV G4LIB_DIR=$G4INSTALL/lib\nENV G4GDMLROOT=$G4INSTALL\n\n# ROOT\nENV ROOT_VERSION=6.36.04\nENV ROOTSYS=/opt/root\n\n# ==========================================\n# Dipendenze base (Qt5 + OpenGL/X11 + ROOT + Python + Vdt)\n# ==========================================\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    build-essential \\\n    cmake \\\n    git \\\n    pkg-config \\\n    ca-certificates \\\n    wget \\\n    curl \\\n    # Geant4 + GDML\n    libxerces-c-dev \\\n    libexpat1-dev \\\n    # Qt5 + OpenGL + X11\n    qtbase5-dev \\\n    qtbase5-dev-tools \\\n    qt5-qmake \\\n    libqt5opengl5-dev \\\n    libx11-dev \\\n    libxmu-dev \\\n    libxi-dev \\\n    libxrandr-dev \\\n    libxinerama-dev \\\n    libxcursor-dev \\\n    libgl1-mesa-dev \\\n    libglu1-mesa-dev \\\n    # runtime Qt/X11\n    libxkbcommon-x11-0 \\\n    libfontconfig1 \\\n    libxrender1 \\\n    libxcb-icccm4 \\\n    libxcb-image0 \\\n    libxcb-keysyms1 \\\n    libxcb-render-util0 \\\n    libxcb-xfixes0 \\\n    libxcb-xinerama0 \\\n    # dipendenze ROOT\n    libxpm-dev \\\n    libxft-dev \\\n    libssl-dev \\\n    libpcre3-dev \\\n    libgsl-dev \\\n    libgraphviz-dev \\\n    libtbb12 \\\n    libtbb-dev \\\n    libvdt-dev \\\n    # Python per gli script di simulazione\n    python3 \\\n    python3-numpy \\\n    python3-matplotlib \\\n    python3-pip \\\n    # utility\n    adduser \\\n &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# ================================\n# Mandatory ReCaS: utente reale\n# ================================\nENV USERNAME=alice\nENV USERID=000001\nENV GROUPID=1234\n\nRUN groupadd -g \"$GROUPID\" \"$USERNAME\" &amp;&amp; \\\n    adduser --disabled-password --gecos '' --uid \"$USERID\" --gid \"$GROUPID\" \"$USERNAME\"\n\n# ==========================================\n# Sorgenti Geant4 11.3.1 (da GitLab CERN)\n# ==========================================\nRUN mkdir -p /opt/geant4-source /tmp/g4 &amp;&amp; \\\n    cd /tmp/g4 &amp;&amp; \\\n    wget https://gitlab.cern.ch/geant4/geant4/-/archive/v11.3.1/geant4-v11.3.1.tar.gz &amp;&amp; \\\n    tar -xzf geant4-v11.3.1.tar.gz -C /opt/geant4-source --strip-components=1 &amp;&amp; \\\n    rm -rf /tmp/g4\n\n# ==========================================\n# Build + install Geant4 (dataset inclusi)\n# ==========================================\nRUN mkdir -p /opt/geant4-build &amp;&amp; cd /opt/geant4-build &amp;&amp; \\\n    cmake ../geant4-source \\\n      -DCMAKE_INSTALL_PREFIX=${G4INSTALL} \\\n      -DGEANT4_BUILD_MULTITHREADED=ON \\\n      -DGEANT4_BUILD_CXXSTD=17 \\\n      -DGEANT4_INSTALL_DATA=ON \\\n      -DGEANT4_INSTALL_EXAMPLES=ON \\\n      -DGEANT4_USE_GDML=ON \\\n      -DGEANT4_USE_SYSTEM_EXPAT=ON \\\n      -DGEANT4_USE_SYSTEM_XERCESC=ON \\\n      -DGEANT4_USE_QT=ON \\\n      -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/Qt5 \\\n      -DGEANT4_USE_OPENGL_X11=ON \\\n      -DGEANT4_USE_XM=OFF \\\n      -DGEANT4_USE_RAYTRACER_X11=ON &amp;&amp; \\\n    cmake --build . -j\"$(nproc)\" &amp;&amp; \\\n    cmake --install . &amp;&amp; \\\n    rm -rf /opt/geant4-build\n\n# =================================================\n# Install ROOT (precompiled per Ubuntu 24.04)\n# =================================================\nRUN cd /opt &amp;&amp; \\\n    wget -O root.tar.gz https://root.cern/download/root_v6.36.04.Linux-ubuntu24.04-x86_64-gcc13.3.tar.gz &amp;&amp; \\\n    tar -xzf root.tar.gz &amp;&amp; \\\n    mv root root-${ROOT_VERSION} &amp;&amp; \\\n    ln -s root-${ROOT_VERSION} root &amp;&amp; \\\n    rm root.tar.gz\n\n# =================================================\n# Linker config + PATH/LD_LIBRARY_PATH globali\n# =================================================\nRUN echo \"${G4LIB_DIR}\"   &gt; /etc/ld.so.conf.d/geant4.conf &amp;&amp; \\\n    echo \"${ROOTSYS}/lib\" &gt; /etc/ld.so.conf.d/root.conf &amp;&amp; \\\n    ldconfig\n\nENV PATH=${G4INSTALL}/bin:${ROOTSYS}/bin:${PATH}\nENV LD_LIBRARY_PATH=${G4LIB_DIR}:${ROOTSYS}/lib\n\n# ===========================\n# Permessi su /opt/geant4\n# ===========================\nRUN chown -R \"$USERNAME:$GROUPID\" \"$G4INSTALL\"\n\n# ======================================================\n# EntryPoint: inizializza Geant4 + ROOT nel modo \"giusto\"\n# ======================================================\nRUN printf '%s\\n' \\\n'#!/bin/bash' \\\n'set -e' \\\n'# --- Geant4 env ---' \\\n'G4_SH=\"${G4INSTALL}/bin/geant4.sh\"' \\\n'if [ -f \"$G4_SH\" ]; then' \\\n'  OLD_G4=\"$(pwd)\"' \\\n'  cd \"$(dirname \"$G4_SH\")\"' \\\n'  . ./geant4.sh' \\\n'  cd \"$OLD_G4\"' \\\n'fi' \\\n'# --- geant4make (opzionale, per vecchi workflow) ---' \\\n'if [ -f \"${G4INSTALL}/share/Geant4-${G4VERSION}/geant4make/geant4make.sh\" ]; then' \\\n'  . \"${G4INSTALL}/share/Geant4-${G4VERSION}/geant4make/geant4make.sh\"' \\\n'fi' \\\n'# --- ROOT env ---' \\\n'if [ -f \"/opt/root/bin/thisroot.sh\" ]; then' \\\n'  OLD_ROOT=\"$(pwd)\"' \\\n'  cd /opt/root' \\\n'  . bin/thisroot.sh' \\\n'  cd \"$OLD_ROOT\"' \\\n'fi' \\\n'# --- Esegui il comando richiesto ---' \\\n'exec \"$@\"' \\\n&gt; /usr/local/bin/geant4-entrypoint.sh &amp;&amp; \\\nchmod +x /usr/local/bin/geant4-entrypoint.sh\n\nWORKDIR /home/$USERNAME\nUSER $USERNAME\n\nENTRYPOINT [\"/usr/local/bin/geant4-entrypoint.sh\"]\nCMD [\"bash\"]\n</code></pre> <p>Best practices</p> <p>Per maggiori indicazioni su come costruire correttamente un Dockerfile e altri suggerimenti si possono visionare i Suggerimenti ufficiali di Docker</p>"},{"location":"apptainer_condor/#riferimenti-ufficiali-recas","title":"Riferimenti ufficiali ReCaS","text":"<p>Per dettagli aggiornati sull\u2019uso di Docker e Dockerfile sul cluster ReCaS si rimanda alla guida ufficiale:</p> <ul> <li>Docker e Dockerfile su ReCaS</li> </ul> <p>La guida presente in questo documento va intesa come complemento operativo orientato agli esempi Geant4/ROOT e all\u2019integrazione con HTCondor, non come sostituto della documentazione ufficiale del cluster.</p>"},{"location":"kubernetes/","title":"Uso di Kubernetes con container su ReCaS","text":""},{"location":"kubernetes/#introduzione-e-abilitazione-dellaccount","title":"Introduzione e abilitazione dell\u2019account","text":"<p>Questa guida affianca la guida su Apptainer/HTCondor: l\u2019idea \u00e8 usare gli stessi container Docker (costruiti e testati come descritto nella guida su Apptainer/Singularity con HTCondor) anche tramite Kubernetes (K8s) sul cluster ReCaS.</p> <p>Nel seguito useremo ancora due utenti fittizi:</p> <ul> <li><code>alice</code>: utente \u201cmanutentore\u201d che costruisce e pubblica le immagini Docker su <code>registry-clustergpu.recas.ba.infn.it</code> (per esempio l\u2019immagine Geant4/ROOT descritta nella sezione sulle immagini).</li> <li><code>bob</code>: utente \u201cnormale\u201d che vuole solo usare quelle immagini per lanciare job di simulazione.</li> </ul> <p>Come per Apptainer/HTCondor, assumeremo che <code>alice</code> abbia gi\u00e0 pubblicato almeno l\u2019immagine:</p> Bash<pre><code>registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n</code></pre> <p>che corrisponde, lato Apptainer, all\u2019immagine <code>.sif</code> <code>G4_v11.3.1.sif</code> elencata nella sezione Immagini Docker / Apptainer disponibili su ReCaS della guida precedente.</p> <p>Con Kubernetes, invece di sottomettere job Condor che avviano Apptainer, <code>bob</code> sottomette Job Kubernetes che lanciano container Docker direttamente sui nodi del cluster. In pratica:</p> <ul> <li>il contenuto del container \u00e8 lo stesso (Ubuntu 24.04 + Geant4 + ROOT + Python);</li> <li>cambia il \u201cmotore\u201d che orchestra i job (Kubernetes invece di HTCondor);</li> <li>la directory <code>/lustrehome</code> rimane la sorgente di tutti i dati persistenti, montata nei Pod.</li> </ul>"},{"location":"kubernetes/#abilitazione-a-kubernetes-in-pratica","title":"Abilitazione a Kubernetes in pratica","text":"<p>Per poter usare Kubernetes, l\u2019account di <code>bob</code> deve essere abilitato e <code>kubectl</code> deve essere configurato correttamente. I passi \u201cufficiali\u201d sono descritti nel dettaglio nella guida ReCaS Job submission using Kubernetes; qui li riassumiamo in forma operativa:</p> <ol> <li> <p>Account HPC/HTC attivo <code>bob</code> deve avere un account ReCaS-Bari per i servizi HPC/HTC e riuscire a collegarsi ai frontend (es. <code>frontend.recas.ba.infn.it</code>) via SSH.</p> </li> <li> <p>Richiesta di accesso a Kubernetes    Una volta attivo l\u2019account, <code>bob</code> apre un ticket tramite il sistema di supporto ReCaS chiedendo l\u2019accesso al cluster Kubernetes HPC/GPU, come indicato nella sezione Access to the service della guida ufficiale (titolo del ticket e dati richiesti sono specificati l\u00ec).</p> </li> <li> <p>Configurazione di <code>kubectl</code> e del <code>kubeconfig</code>    Dopo l\u2019abilitazione a K8s, <code>bob</code> configura il client <code>kubectl</code>:</p> </li> <li>crea <code>~/.kube/config</code> con il template suggerito nella guida, impostando il namespace <code>batch-&lt;username&gt;</code> (ad es. <code>batch-bob</code>);</li> <li> <p>inserisce nel campo <code>token:</code> il proprio access token personale ottenuto via web.</p> </li> <li> <p>Token di accesso    Il token si ottiene autenticandosi con le credenziali ReCaS sull\u2019URL indicato nella guida ufficiale; il token va copiato nel <code>~/.kube/config</code> e ha una durata limitata (quando scade, <code>kubectl</code> smette di funzionare finch\u00e9 non si aggiorna il token). \u00c8 buona pratica proteggere il file di configurazione, ad esempio con:    Bash<pre><code>chmod 700 ~/.kube/config\n</code></pre></p> </li> <li> <p>Verifica della configurazione    Con il token valido e il <code>kubeconfig</code> configurato, <code>bob</code> pu\u00f2 verificare l\u2019accesso con:    Bash<pre><code>kubectl get pod\n</code></pre>    Se la configurazione \u00e8 corretta, il comando risponde con un messaggio del tipo <code>No resources found in batch-bob namespace</code>, che indica che <code>kubectl</code> riesce a parlare con il cluster sul namespace giusto.</p> </li> </ol> <p>Da questo punto in poi, gli esempi nelle sezioni successive presuppongono che <code>bob</code> abbia gi\u00e0: - un account HPC/HTC attivo, - l\u2019accesso al cluster Kubernetes abilitato, - <code>kubectl</code> funzionante sul proprio namespace.</p>"},{"location":"kubernetes/#sec-k8-concepts","title":"Concetti di base di Kubernetes su ReCaS","text":"<p>Per usare Kubernetes in modo consapevole bastano pochi concetti chiave; in questa sezione li adattiamo allo scenario tipico del cluster ReCaS.</p>"},{"location":"kubernetes/#oggetti-principali","title":"Oggetti principali","text":"<ul> <li> <p>Pod   \u00c8 l\u2019unit\u00e0 minima di esecuzione in K8s. Un Pod contiene uno o pi\u00f9 container che condividono la stessa rete e gli stessi volumi.   Negli esempi useremo un solo container per Pod.</p> </li> <li> <p>Job (<code>kind: Job</code>)   \u00c8 l\u2019oggetto Kubernetes pensato per eseguire job batch che terminano.   Un Job crea uno o pi\u00f9 Pod e considera la sua esecuzione completata quando un certo numero di Pod termina con successo.   Negli esempi useremo <code>kind: Job</code> a singolo Pod (<code>completions: 1</code> implicito).</p> </li> <li> <p>Immagine Docker   \u00c8 la stessa immagine che <code>alice</code> ha caricato su <code>registry-clustergpu.recas.ba.infn.it</code> e che abbiamo gi\u00e0 usato con Apptainer.   Nei file YAML verr\u00e0 richiamata con:   YAML<pre><code>image: registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n</code></pre></p> </li> <li> <p>Volume / <code>hostPath</code> verso lustre</p> </li> </ul> <p>In molti casi \u00e8 sufficiente montare l\u2019intera <code>/lustrehome</code> nel Pod, cos\u00ec che:</p> <ul> <li>all\u2019esterno, <code>bob</code> lavora in <code>/lustrehome/bob/k8s_tests/...</code>;</li> <li>dentro il container, il percorso \u00e8 lo stesso.</li> </ul> <p>Questo si traduce in un blocco YAML del tipo:</p> YAML<pre><code>volumes:\n  - name: lustre\n    hostPath:\n      path: /lustrehome\n\ncontainers:\n  - name: main\n    volumeMounts:\n      - name: lustre\n        mountPath: /lustrehome\n</code></pre> <p>Stesso path dentro e fuori dal container</p> <p>Usare <code>/lustrehome</code> come <code>hostPath</code> e <code>mountPath</code> semplifica molto il debug: nei log del Pod vedrai percorsi del tipo <code>/lustrehome/bob/k8s_tests/...</code>, esattamente gli stessi che usi da shell sui frontend (<code>ui-al9</code>, ecc.).</p>"},{"location":"kubernetes/#struttura-tipica-di-un-job-yaml","title":"Struttura tipica di un Job YAML","text":"<p>Tutti gli esempi usano una struttura YAML molto simile:</p> YAML<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: esempio-k8s\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: main\n          image: registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n          workingDir: /lustrehome/bob/k8s_tests/EsK8sX_esempio\n          command: [\"/bin/bash\", \"script_esempio.sh\"]\n          volumeMounts:\n            - name: lustre\n              mountPath: /lustrehome\n      volumes:\n        - name: lustre\n          hostPath:\n            path: /lustrehome\n  backoffLimit: 0\n</code></pre> <p>Elementi importanti:</p> <ul> <li> <p><code>apiVersion: batch/v1</code> e <code>kind: Job</code>   Indicano che stiamo definendo un Job batch.</p> </li> <li> <p><code>metadata.name</code>   Nome del Job; verr\u00e0 usato anche nei nomi dei Pod (con un suffisso random).</p> </li> <li> <p><code>spec.template.spec.containers</code>   Elenco dei container nel Pod.   Qui definiamo:</p> </li> <li>l\u2019immagine Docker da usare;</li> <li>il comando da eseguire (<code>/bin/bash script_esempio.sh</code>);</li> <li>la directory di lavoro (<code>workingDir</code>);</li> <li> <p>i volumi montati.</p> </li> <li> <p><code>restartPolicy: Never</code>   Il Pod non viene riavviato automaticamente se termina (in linea con l\u2019uso batch).</p> </li> <li> <p><code>volumes</code> e <code>volumeMounts</code>   Montano <code>/lustrehome</code> come volume condiviso, cos\u00ec che il job possa leggere/scrivere   negli stessi percorsi visibili dai frontend.</p> </li> <li> <p><code>backoffLimit: 0</code>   Evita che il Job ritenti pi\u00f9 volte se lo script fallisce subito (utile in fase di debug).</p> </li> </ul> <p>Attenzione alle path hard-coded</p> <p>Negli esempi si usano percorsi espliciti come <code>/lustrehome/bob/k8s_tests/...</code>. Ricordarsi di sostituire <code>bob</code> con il proprio username reale e di creare effettivamente le directory necessarie prima di lanciare i Job.</p>"},{"location":"kubernetes/#sec-k8-examples","title":"Esempi pratici Kubernetes","text":"<p>In questa sezione vedremo quattro esempi completi, paralleli agli esempi HTCondor con Apptainer:</p> <ol> <li> <p>Esempio K8s1 \u2013 Geant4 11.3.1 \u201csanity check\u201d    Job minimale che lancia uno script di test dentro il container Geant4 11.3.1 e stampa versione di Geant4, ROOT e Python.</p> </li> <li> <p>Esempio K8s2 \u2013 Build+run dell\u2019esempio Geant4 B5    Job che compila l\u2019esempio Geant4 B5 dentro il container e subito dopo lancia <code>exampleB5</code> con una macro di test.</p> </li> <li> <p>Esempio K8s3 \u2013 Build del progetto CsI-WLS con Geant4 v11    Job che esegue solo la fase di build del progetto CsI-WLS (sorgenti su lustre, build directory nella stessa cartella).</p> </li> <li> <p>Esempio K8s4 \u2013 Batch di run CsI-WLS pilotati da Python    Job che riutilizza la build dell\u2019esempio precedente e lancia un batch di simulazioni tramite uno script Python (macro multiple, output ROOT su lustre).</p> </li> </ol>"},{"location":"kubernetes/#esempio-k8s1-geant4-1131-sanity-check","title":"Esempio K8s1 \u2013 Geant4 11.3.1 \u201csanity check\u201d","text":"<p>Obiettivo: verificare che:</p> <ul> <li>il Job Kubernetes parta correttamente;</li> <li>il container <code>registry-clustergpu.recas.ba.infn.it/marcocecca/geant4:11.3.1</code> sia funzionante;</li> <li>Geant4, ROOT e Python siano visibili dentro il Pod.</li> </ul>"},{"location":"kubernetes/#script-geant4_1131_sanitysh","title":"Script <code>geant4_11.3.1_sanity.sh</code>","text":"Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[K8S-TEST] Start: $(date)\"\necho \"[K8S-TEST] Host:  $(hostname)\"\necho \"[K8S-TEST] User:  $(whoami)\"\necho \"[K8S-TEST] Pwd:   $(pwd)\"\necho\n\necho \"[K8S-TEST] Environment:\"\necho \"  G4INSTALL=${G4INSTALL:-undefined}\"\necho \"  G4VERSION=${G4VERSION:-undefined}\"\necho \"  ROOTSYS=${ROOTSYS:-undefined}\"\necho\n\necho \"[K8S-TEST] Checking Geant4 / ROOT / Python...\"\ncommand -v geant4-config  &gt;/dev/null 2&gt;&amp;1 &amp;&amp; \\\n  geant4-config --version || echo \"geant4-config NOT found\"\n\ncommand -v root-config    &gt;/dev/null 2&gt;&amp;1 &amp;&amp; \\\n  root-config --version   || echo \"root-config NOT found\"\n\ncommand -v python3        &gt;/dev/null 2&gt;&amp;1 &amp;&amp; \\\n  python3 --version       || echo \"python3 NOT found\"\n\necho\npython3 - &lt;&lt; 'EOF'\nimport sys, platform\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Platform:\", platform.platform())\nEOF\n\necho\necho \"[K8S-TEST] Done: $(date)\"\n</code></pre>"},{"location":"kubernetes/#job-geant4_1131_sanityyaml","title":"Job <code>geant4_11.3.1_sanity.yaml</code>","text":"YAML<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: geant4-11-3-1-sanity\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: geant4-11-3-1\n          image: registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n          workingDir: /lustrehome/bob/k8s_tests/EsK8s1_geant4_11.3.1_sanity\n          command: [\"/bin/bash\", \"geant4_11.3.1_sanity.sh\"]\n          volumeMounts:\n            - name: lustre\n              mountPath: /lustrehome\n      volumes:\n        - name: lustre\n          hostPath:\n            path: /lustrehome\n  backoffLimit: 0\n</code></pre>"},{"location":"kubernetes/#comandi-da-lanciare","title":"Comandi da lanciare","text":"<p>Da un frontend (es. <code>ui-al9</code>) come utente <code>bob</code>:</p> Bash<pre><code>cd /lustrehome/bob/k8s_tests/EsK8s1_geant4_11.3.1_sanity\n\n# Crea la directory e copia i template\nmkdir -p /lustrehome/bob/k8s_tests/EsK8s1_geant4_11.3.1_sanity\n# (poi copia qui i file .sh/.yaml)\n\nchmod +x geant4_11.3.1_sanity.sh\n\n# Sottomissione del Job\nkubectl create -f geant4_11.3.1_sanity.yaml\n\n# Stato dei Pod\nkubectl get pods\n\n# Log del Pod (sostituire &lt;pod-name&gt; con quello reale)\nkubectl logs &lt;pod-name&gt;\n\n# Pulizia (Job + Pod associati)\nkubectl delete -f geant4_11.3.1_sanity.yaml\n</code></pre> <p>Verificare il mount di <code>/lustrehome</code></p> <p>Nei log dovresti vedere come <code>pwd</code> qualcosa tipo: <code>/lustrehome/bob/k8s_tests/EsK8s1_geant4_11.3.1_sanity</code>. Se non \u00e8 cos\u00ec, controlla <code>workingDir</code> e i blocchi <code>volumes/volumeMounts</code> nel file YAML.</p>"},{"location":"kubernetes/#esempio-k8s2-buildrun-dellesempio-geant4-b5","title":"Esempio K8s2 \u2013 Build+run dell\u2019esempio Geant4 B5","text":"<p>Questo esempio replica, in ambiente Kubernetes, il flusso build+run dell\u2019esempio B5 descritto nella guida HTCondor:</p> <ul> <li>i sorgenti di B5 sono gi\u00e0 inclusi nell\u2019immagine Docker sotto <code>/opt/geant4/share/Geant4/examples/basic/B5</code>;</li> <li>il job crea una directory di build su lustre;</li> <li>compila <code>exampleB5</code>;</li> <li>esegue <code>exampleB5</code> con una macro di test.</li> </ul>"},{"location":"kubernetes/#script-b5_build_runsh","title":"Script <code>B5_build_run.sh</code>","text":"Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[B5-K8S] Start: $(date)\"\necho \"[B5-K8S] Host:  $(hostname)\"\necho \"[B5-K8S] User:  $(whoami)\"\necho \"[B5-K8S] Pwd:   $(pwd)\"\necho\n\nSRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\"\nBUILD_DIR=\"B5_build_k8s\"\nMACRO=\"${SRC_DIR}/run1.mac\"\n\necho \"[B5-K8S] SRC_DIR   = ${SRC_DIR}\"\necho \"[B5-K8S] BUILD_DIR = ${BUILD_DIR}\"\necho \"[B5-K8S] MACRO     = ${MACRO}\"\necho\n\n# Env (di solito gi\u00e0 inizializzato dall'entrypoint dell'immagine)\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[B5-K8S] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[B5-K8S] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nmkdir -p \"${BUILD_DIR}\"\ncd \"${BUILD_DIR}\"\n\necho \"[B5-K8S] Now in: $(pwd)\"\n\nif [[ -x exampleB5 ]]; then\n  echo \"[B5-K8S] exampleB5 already built, skipping cmake/cmake --build.\"\nelse\n  echo \"[B5-K8S] Running CMake...\"\n  cmake \"${SRC_DIR}\"\n\n  echo \"[B5-K8S] Building...\"\n  cmake --build . -- -j\"$(nproc)\"\nfi\n\nif [[ ! -x exampleB5 ]]; then\n  echo \"[B5-K8S] ERROR: exampleB5 not found after build.\"\n  exit 1\nfi\n\necho\necho \"[B5-K8S] Running exampleB5 with macro: ${MACRO}\"\n./exampleB5 \"${MACRO}\"\n\necho\necho \"[B5-K8S] ROOT files under $(pwd):\"\nfind . -maxdepth 3 -type f -name '*.root' -print || \\\n  echo \"[B5-K8S] Nessun .root trovato.\"\n\necho\necho \"[B5-K8S] Done: $(date)\"\n</code></pre>"},{"location":"kubernetes/#job-b5_build_runyaml","title":"Job <code>B5_build_run.yaml</code>","text":"YAML<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: geant4-11-3-1-b5-build-run\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: geant4-11-3-1-b5\n          image: registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n          workingDir: /lustrehome/bob/k8s_tests/EsK8s2_geant4_11.3.1_B5_build_run\n          command: [\"/bin/bash\", \"B5_build_run.sh\"]\n          resources:\n            requests:\n              cpu: \"4\"\n              memory: \"4Gi\"\n          volumeMounts:\n            - name: lustre\n              mountPath: /lustrehome\n      volumes:\n        - name: lustre\n          hostPath:\n            path: /lustrehome\n  backoffLimit: 0\n</code></pre>"},{"location":"kubernetes/#comandi-da-lanciare_1","title":"Comandi da lanciare","text":"Bash<pre><code>cd /lustrehome/bob/k8s_tests/EsK8s2_geant4_11.3.1_B5_build_run\nmkdir -p /lustrehome/bob/k8s_tests/EsK8s2_geant4_11.3.1_B5_build_run\n\nchmod +x B5_build_run.sh\n\nkubectl create -f B5_build_run.yaml\nkubectl get pods\nkubectl logs &lt;pod-name&gt;\n\n# Al termine\nkubectl delete -f B5_build_run.yaml\n</code></pre> <p>Gli output (in particolare i file <code>.root</code> prodotti da B5) saranno in:</p> Text Only<pre><code>/lustrehome/bob/k8s_tests/EsK8s2_geant4_11.3.1_B5_build_run/B5_build_k8s/\n</code></pre>"},{"location":"kubernetes/#esempio-k8s3-build-del-progetto-csi-wls-con-geant4-v11","title":"Esempio K8s3 \u2013 Build del progetto CsI-WLS con Geant4 v11","text":"<p>In questo esempio <code>bob</code> ha gi\u00e0 una versione del progetto CsI-WLS compatibile con Geant4 v11 (per esempio una copia dei sorgenti in <code>src/</code> come nella sezione HTCondor) e vuole eseguire solo la build dentro il container.</p> <p>Struttura suggerita:</p> Text Only<pre><code>/lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11/\n  src/                 # sorgenti CsI-WLS (CMakeLists.txt, .cc, .hh, OptData, etc.)\n  csi-wls-v11-build.sh\n  csi-wls-v11-build.yaml\n</code></pre>"},{"location":"kubernetes/#script-csi-wls-v11-buildsh","title":"Script <code>csi-wls-v11-build.sh</code>","text":"Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[CsI-K8S-BUILD] Start: $(date)\"\necho \"[CsI-K8S-BUILD] Host:  $(hostname)\"\necho \"[CsI-K8S-BUILD] User:  $(whoami)\"\necho \"[CsI-K8S-BUILD] Pwd:   $(pwd)\"\necho\n\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[CsI-K8S-BUILD] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[CsI-K8S-BUILD] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nBUILD_DIR=\"build\"\nSRC_DIR=\"src\"\n\nmkdir -p \"${BUILD_DIR}\"\ncd \"${BUILD_DIR}\"\n\necho \"[CsI-K8S-BUILD] Now in build dir: $(pwd)\"\n\necho \"[CsI-K8S-BUILD] Configuring with CMake...\"\ncmake \"../${SRC_DIR}\"\n\necho \"[CsI-K8S-BUILD] Building CsI-WLS...\"\ncmake --build . -- -j\"$(nproc)\"\n\nif [[ ! -x CsI-WLS ]]; then\n  echo \"[CsI-K8S-BUILD] ERROR: CsI-WLS binary not found after build.\"\n  exit 1\nfi\n\necho\necho \"[CsI-K8S-BUILD] Build completed, CsI-WLS is available in $(pwd)\"\nls -l CsI-WLS || true\n\necho\necho \"[CsI-K8S-BUILD] Done: $(date)\"\n</code></pre>"},{"location":"kubernetes/#job-csi-wls-v11-buildyaml","title":"Job <code>csi-wls-v11-build.yaml</code>","text":"YAML<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: csi-wls-v11-build\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: csi-wls-v11-build\n          image: registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n          workingDir: /lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11\n          command: [\"/bin/bash\", \"csi-wls-v11-build.sh\"]\n          resources:\n            requests:\n              cpu: \"4\"\n              memory: \"4Gi\"\n          volumeMounts:\n            - name: lustre\n              mountPath: /lustrehome\n      volumes:\n        - name: lustre\n          hostPath:\n            path: /lustrehome\n  backoffLimit: 0\n</code></pre>"},{"location":"kubernetes/#comandi-da-lanciare_2","title":"Comandi da lanciare","text":"Bash<pre><code>cd /lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11\nchmod +x csi-wls-v11-build.sh\n\nkubectl create -f csi-wls-v11-build.yaml\nkubectl get pods\nkubectl logs &lt;pod-name&gt;\n\n# Pulizia\nkubectl delete -f csi-wls-v11-build.yaml\n</code></pre> <p>Al termine, l\u2019eseguibile <code>CsI-WLS</code> sar\u00e0 disponibile in:</p> Text Only<pre><code>/lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11/build/\n</code></pre>"},{"location":"kubernetes/#esempio-k8s4-batch-di-run-csi-wls-pilotati-da-python","title":"Esempio K8s4 \u2013 Batch di run CsI-WLS pilotati da Python","text":"<p>Qui estendiamo l\u2019esempio K8s3: assumiamo che la build sia gi\u00e0 stata eseguita (l\u2019eseguibile <code>CsI-WLS</code> esiste in <code>build/</code>) e vogliamo far partire un batch di simulazioni pilotate da uno script Python, in stile HTCondor esempio 5.</p> <p>Struttura suggerita:</p> Text Only<pre><code>/lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11/\n  src/\n    ...                 # sorgenti CsI-WLS\n    run_electrons_batch.py\n  build/\n    CsI-WLS             # ottenuto da K8s3\n  csi-wls-v11-run-batch.sh\n  csi-wls-v11-run-batch.yaml\n</code></pre>"},{"location":"kubernetes/#script-python-run_electrons_batchpy","title":"Script Python <code>run_electrons_batch.py</code>","text":"<p>Esempio minimale che genera N macro, ognuna con un seed diverso, e lancia <code>./CsI-WLS</code> per ciascuna:</p> Python<pre><code>import numpy as np\nimport os\n\nDIR_MAC = \"./mac_electron\"\nDIR_ROOT = \"rootOutput_electron\"\n\nN_EVENTS = 10\nsubdir = \"random_rectangular_source_200keV\"\nnfiles = 50\n\nfor k in range(nfiles):\n    seed1, seed2 = np.random.randint(0, 2**32, size=2)\n\n    mac = (f\"{DIR_MAC}/{subdir}/\"\n           f\"random_rectangular_source_electron_ene_\"\n           f\"200keV_ly1_n{k}.mac\")\n    root = mac.replace(DIR_MAC, DIR_ROOT).replace(\".mac\", \"\")\n\n    os.makedirs(os.path.dirname(mac),  exist_ok=True)\n    os.makedirs(os.path.dirname(root), exist_ok=True)\n\n    with open(mac, \"w\") as f:\n        f.write(\n            \"/run/initialize\\n\"\n            \"/tracking/verbose 0\\n\"\n            \"/gps/particle e-\\n\"\n            \"/gps/position 0 0 0 mm\\n\"\n            \"/gps/direction 0 0 -1\\n\"\n            f\"/random/setSeeds [{seed1} {seed2}]\\n\"\n            \"/gps/pos/type Plane\\n\"\n            \"/gps/pos/shape Rectangle\\n\"\n            \"/gps/pos/halfx 220 mm\\n\"\n            \"/gps/pos/halfy 220 mm\\n\"\n            \"/gps/ene/mono 200 keV\\n\"\n            f\"/RunManager/NameOfOutputFile {root}\\n\"\n            f\"/run/beamOn {N_EVENTS}\\n\"\n        )\n\n    os.system(f\"./CsI-WLS {mac}\")\n\nprint(f\"\\n{N_EVENTS * nfiles} eventi simulati. \"\n      f\"Macro in {DIR_MAC}/{subdir}, output ROOT in {DIR_ROOT}/{subdir}\")\n</code></pre>"},{"location":"kubernetes/#script-csi-wls-v11-run-batchsh","title":"Script <code>csi-wls-v11-run-batch.sh</code>","text":"Bash<pre><code>#!/bin/bash\nset -euo pipefail\n\necho \"[CsI-K8S-RUN] Start: $(date)\"\necho \"[CsI-K8S-RUN] Host:  $(hostname)\"\necho \"[CsI-K8S-RUN] User:  $(whoami)\"\necho \"[CsI-K8S-RUN] Pwd:   $(pwd)\"\necho\n\nif [[ -f /opt/geant4/bin/geant4.sh ]]; then\n  echo \"[CsI-K8S-RUN] Sourcing Geant4...\"\n  source /opt/geant4/bin/geant4.sh\nfi\nif [[ -f /opt/root/bin/thisroot.sh ]]; then\n  echo \"[CsI-K8S-RUN] Sourcing ROOT...\"\n  source /opt/root/bin/thisroot.sh\nfi\n\nBUILD_DIR=\"build\"\n\nif [[ ! -d \"${BUILD_DIR}\" ]]; then\n  echo \"[CsI-K8S-RUN] ERROR: build directory '${BUILD_DIR}' not found.\"\n  exit 1\nfi\n\ncd \"${BUILD_DIR}\"\necho \"[CsI-K8S-RUN] Now in build dir: $(pwd)\"\n\nif [[ ! -x CsI-WLS ]]; then\n  echo \"[CsI-K8S-RUN] ERROR: CsI-WLS binary not found in $(pwd).\"\n  exit 1\nfi\n\necho\necho \"[CsI-K8S-RUN] Running Python batch script...\"\npython3 ../src/run_electrons_batch.py\n\necho\necho \"[CsI-K8S-RUN] ROOT files under rootOutput_electron/:\"\nfind rootOutput_electron -maxdepth 3 -type f -name '*.root' -print \\\n  || echo \"[CsI-K8S-RUN] Nessun .root trovato.\"\n\necho\necho \"[CsI-K8S-RUN] Done: $(date)\"\n</code></pre>"},{"location":"kubernetes/#job-csi-wls-v11-run-batchyaml","title":"Job <code>csi-wls-v11-run-batch.yaml</code>","text":"YAML<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: csi-wls-v11-run-batch\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: csi-wls-v11-run-batch\n          image: registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1\n          workingDir: /lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11\n          command: [\"/bin/bash\", \"csi-wls-v11-run-batch.sh\"]\n          resources:\n            requests:\n              cpu: \"1\"\n              memory: \"4Gi\"\n          volumeMounts:\n            - name: lustre\n              mountPath: /lustrehome\n      volumes:\n        - name: lustre\n          hostPath:\n            path: /lustrehome\n  backoffLimit: 0\n</code></pre>"},{"location":"kubernetes/#comandi-da-lanciare_3","title":"Comandi da lanciare","text":"Bash<pre><code>cd /lustrehome/bob/k8s_tests/EsK8s3_CsI_WLS_v11\nchmod +x csi-wls-v11-run-batch.sh\n\nkubectl create -f csi-wls-v11-run-batch.yaml\nkubectl get pods\nkubectl logs &lt;pod-name&gt;\n\n# Pulizia\nkubectl delete -f csi-wls-v11-run-batch.yaml\n</code></pre> <p>Assicurarsi che la build esista prima del batch</p> <p>L\u2019esempio K8s4 presuppone che: - la directory <code>build/</code> esista; - l\u2019eseguibile <code>CsI-WLS</code> sia stato prodotto da K8s3.</p> <p>Se il Job fallisce immediatamente con un errore tipo \u201cCsI-WLS binary not found\u201d, rilanciare prima il Job di build (<code>csi-wls-v11-build.yaml</code>).</p>"},{"location":"kubernetes/#sec-k8-commands","title":"Comandi essenziali di Kubernetes","text":"<p>Questa sezione riassume i comandi <code>kubectl</code> pi\u00f9 usati nello scenario degli esempi.</p>"},{"location":"kubernetes/#gestione-dei-job-e-dei-pod","title":"Gestione dei Job e dei Pod","text":"Bash<pre><code># Creare un Job da un file YAML\nkubectl create -f job_esempio.yaml\n\n# Elenco dei Job nel namespace corrente\nkubectl get jobs\n\n# Dettagli di un Job specifico\nkubectl describe job nome-job\n\n# Elenco dei Pod (inclusi quelli creati dai Job)\nkubectl get pods\n\n# Dettagli di un Pod\nkubectl describe pod nome-pod\n</code></pre>"},{"location":"kubernetes/#log-e-debug","title":"Log e debug","text":"Bash<pre><code># Log standard di un Pod\nkubectl logs nome-pod\n\n# Log in streaming (finch\u00e9 il Pod \u00e8 in esecuzione)\nkubectl logs -f nome-pod\n\n# Shell interattiva dentro il container del Pod\nkubectl exec -it nome-pod -- /bin/bash\n</code></pre>"},{"location":"kubernetes/#pulizia","title":"Pulizia","text":"Bash<pre><code># Eliminare un Job (e i Pod associati)\nkubectl delete -f job_esempio.yaml\n# oppure\nkubectl delete job nome-job\n\n# Eliminare un Pod singolo (se necessario)\nkubectl delete pod nome-pod\n</code></pre> <p>Allineare naming e directory</p> <p>Mantenere un mapping chiaro tra: - directory su lustre (<code>EsK8s1_...</code>, <code>EsK8s2_...</code>, ecc.), - nomi dei Job (<code>metadata.name</code>), - nomi dei file YAML (<code>EsK8sX_*.yaml</code>).</p> <p>Questo rende molto pi\u00f9 semplice capire cosa cancellare e dove andare a leggere gli output.</p>"},{"location":"kubernetes/#sec-k8-refs","title":"Appendice \u2013 Riferimenti ufficiali e documentazione Kubernetes","text":"<p>Per approfondimenti oltre gli esempi di questa guida ci si pu\u00f2 rifare alla Documentazione ufficiale Kubernetes, introduttiva e di riferimento su concetti come Pod, Job, volumi, risorse, ecc, e alla guida ufficiale di ReCaS su Kubernetes che contiene istruzioni aggiornate per l\u2019abilitazione degli utenti, esempi di configurazione <code>kubectl</code> per il cluster e policy su limiti di CPU/RAM e best practice per l\u2019uso di K8s in ambiente HPC.</p> <p>La parte Kubernetes di questo documento va intesa come complemento operativo agli esempi Geant4/ROOT/CsI-WLS, non come sostituto della documentazione ufficiale.</p>"}]}