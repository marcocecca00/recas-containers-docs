{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Uso di immagini Apptainer/Singularity con HTCondor su ReCaS \u00b6 Introduzione \u00b6 Questa guida descrive in modo operativo come usare immagini Apptainer/Singularity (file .sif ) insieme a HTCondor sul cluster ReCaS. L\u2019idea di fondo \u00e8 che nel gruppo ci sia almeno un utente \u201cmanutentore\u201d (che chiameremo alice ) che possa costruire immagini Docker su una macchina dedicata (ad esempio una macchina con Docker come tesla02 ), convertirle in immagini Apptainer/Singularity e metterle a disposizione di tutti in una posizione condivisa su lustre. Gli altri utenti (ad esempio bob ) non devono occuparsi della parte Docker: si limitano a usare le immagini .sif gi\u00e0 pronte all\u2019interno dei job Condor, tramite symlink verso la directory condivisa di alice . Nel seguito useremo come esempio un utente chiamato bob per i job Condor, mentre alice rappresenter\u00e0 l\u2019utente manutentore che ospita le immagini condivise. Per rendere gli esempi concreti, si assume che siano gi\u00e0 presenti due immagini Apptainer nella directory condivisa di alice : /lustrehome/alice/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif /lustrehome/alice/apptainer_images/G4_v11.3.1.sif Queste immagini contengono Ubuntu 24.04, Geant4, ROOT, Python3 con numpy e matplotlib. In particolare l\u2019immagine G4_v10.6.3_NOMULTITHREAD.sif ha il multithreading disattivato per Geant4 ed \u00e8 adatta ad applicazioni che richiedono esecuzione single-thread. La sezione Concetti di base: container, Apptainer e HTCondor introduce i concetti di base su container, Apptainer e HTCondor, mentre la sezione Organizzazione delle directory propone una convenzione semplice per organizzare le directory su lustre dal punto di vista di bob . La sezione Esempi illustra cinque esempi completi di utilizzo: test dell\u2019immagine, build dell\u2019esempio B5 di Geant4, run di B5, build+run in un unico job e un caso reale con una simulazione Geant di una tile scintillante tra due piani di fibre WLS (CsI-WLS) e Python. La sezione Costruzione di un\u2019immagine Docker e conversione in SIF mostra come costruire e convertire immagini Docker in .sif , mentre la sezione Prospettive: uso di Kubernetes con container \u00e8 prevista come estensione futura. In Appendice, la sezione Dockerfile di esempio per ambiente Geant4/ROOT contiene un Dockerfile di esempio. Concetti di base: container, Apptainer e HTCondor \u00b6 Prima di entrare negli esempi conviene chiarire cosa si intende per container e come Apptainer interagisce con HTCondor nel contesto del cluster ReCaS. Un container \u00e8 un ambiente software isolato, definito da un\u2019immagine che contiene un sistema operativo minimale (ad esempio Ubuntu), le librerie e le applicazioni necessarie. Quando si avvia un container, il programma viene eseguito con quell\u2019ambiente software, indipendentemente dal sistema operativo del nodo fisico. Nel nostro caso un\u2019immagine .sif contiene Geant4, ROOT, Python e le relative dipendenze, cos\u00ec che un job Condor non deve installare o configurare nulla: trova tutto gi\u00e0 predisposto. Su ReCaS i container sono gestiti da Apptainer (discendente di Singularity), progettato per ambienti HPC multiutente. Un\u2019immagine Apptainer \u00e8 un file in sola lettura; durante il job, Apptainer monta il filesystem dell\u2019immagine e allo stesso tempo monta la directory di lavoro dell\u2019utente, in modo che il programma possa leggere e scrivere i propri file su lustre. Questo approccio \u00e8 pi\u00f9 leggero di una macchina virtuale, perch\u00e9 il kernel del sistema \u00e8 condiviso e si avvia solo lo strato utente. HTCondor si occupa di individuare i worker node disponibili, preparare la directory di lavoro e avviare Apptainer. Dal punto di vista dell\u2019utente, la cosa fondamentale \u00e8 capire il ruolo di tre parametri nel file di submit: initialdir , executable e container_image . La direttiva initialdir indica la directory sul filesystem di lustre che rappresenta la cartella di lavoro del job. Condor monta questa directory nel container come current working directory (CWD), quindi tutto ci\u00f2 che viene scritto in CWD o in sottocartelle relative finisce direttamente in questa directory su lustre. La direttiva executable indica lo script o l\u2019eseguibile che verr\u00e0 lanciato all\u2019interno del container. Deve trovarsi nella initialdir o in una sua sottocartella ed \u00e8 specificato nel file di submit con un percorso relativo. La direttiva container_image indica quale immagine Apptainer usare. I test effettuati sul cluster hanno mostrato un comportamento pratico importante: Condor si aspetta che il valore di container_image sia il nome di un file presente nella initialdir . Per questa ragione, anche se l\u2019immagine \u201creale\u201d vive in una directory centrale, per ogni job conviene creare nella initialdir un symlink locale all\u2019immagine e poi usare nel submit il nome del symlink (ad esempio un symlink a /lustrehome/alice/apptainer_images/immagine.sif ). Un esempio tipico \u00e8 il seguente. Nella directory del job di bob si crea un link all\u2019immagine condivisa di alice : ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif e nel file di submit si scrive: container_image = G4_v11.3.1.sif In questo modo Condor trova il file G4_v11.3.1.sif nella initialdir , avvia Apptainer con quell\u2019immagine e monta la initialdir all\u2019interno del container. Da quel momento in poi lo script executable viene eseguito dentro il container e la directory di lavoro corrisponde alla directory dell\u2019utente su lustre. Gli esempi pratici della sezione Esempi non fanno altro che declinare questo schema base in casi d\u2019uso via via pi\u00f9 complessi. Organizzazione delle directory \u00b6 Per lavorare in modo ordinato conviene scegliere una convenzione semplice all\u2019interno della propria home su lustre. Nel caso di bob , la directory di riferimento per i job \u00e8 /lustrehome/bob , mentre l\u2019utente manutentore alice usa /lustrehome/alice per ospitare le immagini condivise. Le immagini Apptainer condivise dal manutentore possono essere raccolte in una directory dedicata, ad esempio /lustrehome/alice/apptainer_images . In questa directory si collocano i file .sif che alice ha costruito o recuperato. Nel nostro esempio vi si trovano G4_v11.3.1.sif e G4_v10.6.3_NOMULTITHREAD.sif . Per gli esempi e i job Condor di bob si pu\u00f2 usare una directory condor_tests . All\u2019interno di condor_tests \u00e8 utile creare sottodirectory dedicate per ciascun tipo di job. Ogni directory contiene i file di submit .csi , gli script .sh , un link locale all\u2019immagine .sif (proveniente da /lustrehome/alice/apptainer_images ) e una sottocartella logs/ per gli output di Condor. Questa struttura rende chiaro dove si trova il codice sorgente, dove viene compilato il programma e dove finiscono i file prodotti dai job Condor, seguendo lo schema introdotto in Concetti di base: container, Apptainer e HTCondor e utilizzato in tutti gli esempi successivi. Esempi \u00b6 In questa sezione sono riportati cinque esempi completi che illustrano come utilizzare immagini Apptainer/Singularity in combinazione con HTCondor. Gli esempi seguono un ordine progressivo, dal test pi\u00f9 semplice fino a un caso realistico con un progetto Geant4 personalizzato: Esempio 1 \u2013 test minimale dell\u2019immagine .sif per verificare la versione di Geant4, ROOT e Python e controllare che il container venga avviato correttamente su un worker node; Esempio 2 \u2013 compilazione dell\u2019esempio Geant4 B5 all\u2019interno del container utilizzando CMake; Esempio 3 \u2013 esecuzione di un binario Geant4 precompilato con una macro, in un job dedicato; Esempio 4 \u2013 compilazione ed esecuzione dell\u2019esempio B5 nello stesso job HTCondor, utile quando si vuole una build \u201cpulita\u201d per ogni run; Esempio 5 \u2013 caso realistico con il progetto CsI-WLS, che prevede build con CMake e un batch di simulazioni pilotato da uno script Python. I template completi degli script .sh e dei file di submit .csi utilizzati negli esempi successivi sono disponibili al seguente link: https://politecnicobari-my.sharepoint.com/:f:/g/personal/m_cecca1_phd_poliba_it/IgB8X8DaFI3QTaVAEtIk1-7vAavKvy_XEBCvTr6rky2CQf8?e=si0j3J Esempio 1: test dell'immagine Geant4 \u00b6 Il primo esempio ha lo scopo di verificare che l\u2019immagine G4_v11.3.1.sif funzioni correttamente su un worker node. L\u2019obiettivo \u00e8 sapere su quale nodo gira il job, quali versioni di Geant4, ROOT e Python sono visibili dall\u2019interno del container e se l\u2019ambiente \u00e8 coerente. Si inizia creando la directory del test e una sottocartella per i log: cd /lustrehome/bob mkdir -p condor_tests/test_container/logs cd condor_tests/test_container Si crea un link all\u2019immagine condivisa di alice : ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Lo script test_container.sh , che sar\u00e0 eseguito dentro il container, pu\u00f2 essere definito come segue: #!/bin/bash set -euo pipefail echo \"[TEST] Start: $(date)\" echo \"[TEST] Host: $(hostname)\" echo \"[TEST] User: $(whoami)\" echo \"[TEST] Pwd: $(pwd)\" echo echo \"[TEST] Environment snippet:\" echo \" G4INSTALL=${G4INSTALL:-undefined}\" echo \" G4VERSION=${G4VERSION:-undefined}\" echo \" ROOTSYS=${ROOTSYS:-undefined}\" echo echo \"[TEST] Checking Geant4 / ROOT / Python...\" command -v geant4-config >/dev/null 2>&1 && \\ geant4-config --version || echo \"geant4-config NOT found\" command -v root-config >/dev/null 2>&1 && \\ root-config --version || echo \"root-config NOT found\" command -v python3 >/dev/null 2>&1 && \\ python3 --version || echo \"python3 NOT found\" echo python3 - << 'EOF' import sys, platform print(\"Python:\", sys.version.split()[0]) print(\"Platform:\", platform.platform()) EOF echo echo \"[TEST] Done: $(date)\" Lo script va reso eseguibile: chmod +x test_container.sh Il file di submit test_container.csi specifica la directory iniziale, lo script da eseguire e l\u2019immagine da usare: universe = vanilla initialdir = /lustrehome/bob/condor_tests/test_container executable = test_container.sh arguments = container_image = G4_v11.3.1.sif request_cpus = 1 request_memory = 1 GB request_disk = 4 GB output = logs/test_$(ClusterId).$(ProcId).out error = logs/test_$(ClusterId).$(ProcId).err log = logs/test_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene con: condor_submit test_container.csi Quando il job \u00e8 completato, il file logs/test_...out contiene le informazioni stampate dallo script: il nome del worker node, l\u2019utente, la directory di lavoro, le variabili di ambiente e le versioni dei software principali. Questo conferma che l\u2019immagine \u00e8 correttamente utilizzabile con HTCondor secondo lo schema introdotto in Concetti di base: container, Apptainer e HTCondor . Esempio 2: build dell\u2019esempio B5 di Geant4 \u00b6 Il secondo esempio mostra come compilare l\u2019esempio B5 di Geant4 usando l\u2019immagine G4_v11.3.1.sif . L\u2019obiettivo \u00e8 ottenere l\u2019eseguibile exampleB5 in una directory di build gestita dal job. Si crea la directory del test di build: cd /lustrehome/bob mkdir -p condor_tests/build_B5_11.3.1/logs cd condor_tests/build_B5_11.3.1 Si collega l\u2019immagine condivisa: ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Lo script build_B5_exec.sh viene eseguito dentro il container e si occupa di configurare e compilare il progetto: #!/bin/bash set -euo pipefail echo \"[BUILD] Start: $(date)\" echo \"[BUILD] Host: $(hostname)\" echo \"[BUILD] User: $(whoami)\" echo \"[BUILD] Pwd: $(pwd)\" echo SRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\" BUILD_DIR=\"B5_build_condor\" echo \"[BUILD] SRC_DIR = ${SRC_DIR}\" echo \"[BUILD] BUILD_DIR = ${BUILD_DIR}\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[BUILD] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[BUILD] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi mkdir -p \"${BUILD_DIR}\" cd \"${BUILD_DIR}\" echo \"[BUILD] Now in: $(pwd)\" echo \"[BUILD] Running CMake...\" cmake \"${SRC_DIR}\" echo \"[BUILD] Building...\" cmake --build . -- -j\"$(nproc)\" echo echo \"[BUILD] Done: $(date)\" Lo script viene reso eseguibile: chmod +x build_B5_exec.sh Il file di submit build_B5_11.3.1.csi \u00e8: universe = vanilla initialdir = /lustrehome/bob/condor_tests/build_B5_11.3.1 executable = build_B5_exec.sh arguments = container_image = G4_v11.3.1.sif request_cpus = 4 request_memory = 4 GB request_disk = 8 GB output = logs/build_$(ClusterId).$(ProcId).out error = logs/build_$(ClusterId).$(ProcId).err log = logs/build_$(ClusterId).$(ProcId).log queue 1 Il job si sottomette con: condor_submit build_B5_11.3.1.csi Al termine della compilazione, nella directory /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor si trovano i file di CMake e l\u2019eseguibile exampleB5 . Lo standard output del job contiene i messaggi di CMake e l\u2019esito del build, che verr\u00e0 riutilizzato nella sezione Esempio 3: run dell\u2019esempio B5 . Esempio 3: run dell\u2019esempio B5 \u00b6 Il terzo esempio riutilizza l\u2019eseguibile exampleB5 compilato in Esempio 2: build dell\u2019esempio B5 di Geant4 e mostra come preparare una directory di run separata, in cui copiare l\u2019eseguibile e le macro e lanciare la simulazione. Si crea la directory per il run: cd /lustrehome/bob mkdir -p condor_tests/run_B5_11.3.1/logs cd condor_tests/run_B5_11.3.1 Si collega l\u2019immagine: ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Si copiano l\u2019eseguibile e la macro run1.mac dalla build: cp /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor/exampleB5 . cp /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor/run1.mac . Lo script run_B5_exec.sh lancia l\u2019eseguibile con la macro: #!/bin/bash set -euo pipefail if [[ $# -lt 2 ]]; then echo \"Usage: $0 <exec_rel_path> <macro_rel_path>\" echo \"Example: $0 exampleB5 run1.mac\" exit 1 fi EXEC_REL=\"$1\" MACRO_REL=\"$2\" echo \"[RUN] Start: $(date)\" echo \"[RUN] Host: $(hostname)\" echo \"[RUN] User: $(whoami)\" echo \"[RUN] Pwd: $(pwd)\" echo \"[RUN] Exec: ${EXEC_REL}\" echo \"[RUN] Macro: ${MACRO_REL}\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[RUN] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[RUN] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi EXEC_PATH=\"./${EXEC_REL}\" MACRO_PATH=\"./${MACRO_REL}\" if [[ ! -x \"${EXEC_PATH}\" ]]; then echo \"[RUN] ERROR: executable not found or not executable: ${EXEC_PATH}\" exit 1 fi if [[ ! -f \"${MACRO_PATH}\" ]]; then echo \"[RUN] ERROR: macro not found: ${MACRO_PATH}\" exit 1 fi echo \"[RUN] Launching: ${EXEC_PATH} ${MACRO_PATH}\" \"${EXEC_PATH}\" \"${MACRO_PATH}\" echo echo \"[RUN] Done: $(date)\" Lo script va reso eseguibile: chmod +x run_B5_exec.sh Il file di submit run_B5_11.3.1.csi utilizza lo script, l\u2019immagine e specifica le risorse richieste: universe = vanilla initialdir = /lustrehome/bob/condor_tests/run_B5_11.3.1 executable = run_B5_exec.sh arguments = exampleB5 run1.mac container_image = G4_v11.3.1.sif request_cpus = 1 request_memory = 2 GB request_disk = 4 GB output = logs/run_$(ClusterId).$(ProcId).out error = logs/run_$(ClusterId).$(ProcId).err log = logs/run_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene come nei casi precedenti: condor_submit run_B5_11.3.1.csi Gli output prodotti da B5 vengono scritti nella directory /lustrehome/bob/condor_tests/run_B5_11.3.1 , che corrisponde alla directory di lavoro del job dentro il container, e rimangono quindi disponibili all\u2019utente anche dopo la fine dell\u2019esecuzione. Esempio 4: build e run di B5 in un unico job \u00b6 In alcune situazioni \u00e8 comodo compilare il codice e far partire subito il run all\u2019interno dello stesso job Condor. Questo permette di avere un singolo file di submit per l\u2019intera catena e di garantire che il run utilizzi esattamente la build prodotta nel job. Per questo esempio si crea una nuova directory: cd /lustrehome/bob mkdir -p condor_tests/build_run_B5_11.3.1/logs cd condor_tests/build_run_B5_11.3.1 Si collega l\u2019immagine Geant4 11.3.1: ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Lo script build_run_B5_exec.sh esegue prima la compilazione dell\u2019esempio B5 e subito dopo l\u2019eseguibile con una macro di esempio: #!/bin/bash set -euo pipefail echo \"[BUILD+RUN] Start: $(date)\" echo \"[BUILD+RUN] Host: $(hostname)\" echo \"[BUILD+RUN] User: $(whoami)\" echo \"[BUILD+RUN] Pwd: $(pwd)\" echo SRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\" BUILD_DIR=\"B5_build_condor\" MACRO=\"${SRC_DIR}/run1.mac\" echo \"[BUILD+RUN] SRC_DIR = ${SRC_DIR}\" echo \"[BUILD+RUN] BUILD_DIR = ${BUILD_DIR}\" echo \"[BUILD+RUN] MACRO = ${MACRO}\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[BUILD+RUN] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[BUILD+RUN] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi mkdir -p \"${BUILD_DIR}\" cd \"${BUILD_DIR}\" echo \"[BUILD+RUN] Now in: $(pwd)\" if [[ -x exampleB5 ]]; then echo \"[BUILD+RUN] exampleB5 already built, skipping CMake/cmake --build.\" else echo \"[BUILD+RUN] Running CMake...\" cmake \"${SRC_DIR}\" echo \"[BUILD+RUN] Building...\" cmake --build . -- -j\"$(nproc)\" fi if [[ ! -x exampleB5 ]]; then echo \"[BUILD+RUN] ERROR: build failed, exampleB5 not found.\" exit 1 fi echo echo \"[BUILD+RUN] Running exampleB5 with macro: ${MACRO}\" ./exampleB5 \"${MACRO}\" echo echo \"[BUILD+RUN] ROOT files under $(pwd):\" find . -maxdepth 3 -type f -name '*.root' -print || \\ echo \"[BUILD+RUN] Nessun .root trovato.\" echo echo \"[BUILD+RUN] Done: $(date)\" Lo script viene reso eseguibile: chmod +x build_run_B5_exec.sh Il file di submit build_run_B5_11.3.1.csi \u00e8 il seguente: universe = vanilla initialdir = /lustrehome/bob/condor_tests/build_run_B5_11.3.1 executable = build_run_B5_exec.sh arguments = container_image = G4_v11.3.1.sif request_cpus = 4 request_memory = 4 GB request_disk = 8 GB output = logs/build_run_$(ClusterId).$(ProcId).out error = logs/build_run_$(ClusterId).$(ProcId).err log = logs/build_run_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene con: condor_submit build_run_B5_11.3.1.csi Alla conclusione del job, la directory B5_build_condor contiene sia i file generati da CMake, sia l\u2019eseguibile exampleB5 , sia i file di output del run (ad esempio file ROOT). Tutti questi file risiedono in /lustrehome/bob/condor_tests/build_run_B5_11.3.1/B5_build_condor e sono quindi accessibili per analisi successive. Esempio 5: progetto CsI-WLS con Python \u00b6 L\u2019ultimo esempio mostra una situazione pi\u00f9 vicina a un caso reale, in cui un\u2019applicazione Geant4 custom (CsI-WLS) viene compilata da sorgente con CMake e poi eseguita molte volte con parametri diversi, gestiti da uno script Python. Per questo caso si sfrutta l\u2019immagine G4_v10.6.3_NOMULTITHREAD.sif , che contiene Geant4 10.6.3 senza multithreading, ROOT 6.36.4 e Python3 con le librerie principali. Si assume che il progetto CsI-WLS esista gi\u00e0 in una directory di sviluppo, ad esempio /lustrehome/bob/ADAPT/simulation/CsI-WLS_v1.2.2 , che contiene una sottodirectory src con i file CMake e il codice. Per rendere il job auto-contenuto dentro condor_tests si copia solo la directory src : cd /lustrehome/bob/condor_tests mkdir -p CsI-WLS_v1.2.2 cp -r /lustrehome/bob/ADAPT/simulation/CsI-WLS_v1.2.2/src CsI-WLS_v1.2.2/ cd CsI-WLS_v1.2.2 mkdir -p logs Si crea il link all\u2019immagine senza multithreading di alice : ln -sf /lustrehome/alice/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif \\ G4_v10.6.3_NOMULTITHREAD.sif Nella directory source (nel testo LaTeX originale si alternano source / src ; qui assumiamo che lo script Python sia nella directory con i sorgenti) si definisce lo script Python che si aspetta di essere eseguito da dentro build (dove esiste ./CsI-WLS ). Questo script genera un certo numero di macro, ognuna con un seed diverso, e per ciascuna macro lancia l\u2019eseguibile: import numpy as np import os import matplotlib.pyplot as plt # non usato, ma non da fastidio DIR_MAC = \"./mac_electron\" DIR_ROOT = \"rootOutput_electron\" N_EVENTS = 10 subdir = \"random_rectangular_source_200keV\" nfiles = 50 for k in range(nfiles): seed1, seed2 = np.random.randint(0, 2**32, size=2) mac = (f\"{DIR_MAC}/{subdir}/\" f\"random_rectangular_source_electron_ene_\" f\"200keV_ly1_n{k}.mac\") root = mac.replace(DIR_MAC, DIR_ROOT).replace(\".mac\", \"\") os.makedirs(os.path.dirname(mac), exist_ok=True) os.makedirs(os.path.dirname(root), exist_ok=True) with open(mac, \"w\") as f: f.write( \"/run/initialize\\n\" \"/tracking/verbose 0\\n\" \"/gps/particle e-\\n\" \"/gps/position 0 0 0 mm\\n\" \"/gps/direction 0 0 -1\\n\" f\"/random/setSeeds [{seed1} {seed2}]\\n\" \"/gps/pos/type Plane\\n\" \"/gps/pos/shape Rectangle\\n\" \"/gps/pos/halfx 220 mm\\n\" \"/gps/pos/halfy 220 mm\\n\" \"/gps/ene/mono 200 keV\\n\" f\"/RunManager/NameOfOutputFile {root}\\n\" f\"/run/beamOn {N_EVENTS}\\n\" ) os.system(f\"./CsI-WLS {mac}\") print(f\"\\n {N_EVENTS*nfiles} eventi salvati in \" f\"{DIR_MAC}/{subdir} e simulati con ./CsI-WLS\") Questo file pu\u00f2 essere salvato come src/run_electrons_batch.py (o source/run_electrons_batch.py , a seconda della struttura del progetto) all\u2019interno della copia di CsI-WLS sotto condor_tests . Nella root della directory CsI-WLS_v1.2.2 si definisce lo script run_CsI_WLS_electron_batch.sh , che compila il progetto nella directory build e poi lancia lo script Python: #!/bin/bash set -euo pipefail echo \"[PYRUN] Start: $(date)\" echo \"[PYRUN] Host: $(hostname)\" echo \"[PYRUN] User: $(whoami)\" echo \"[PYRUN] Pwd: $(pwd)\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[PYRUN] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[PYRUN] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi BUILD_DIR=\"build\" SRC_DIR=\"source\" mkdir -p \"${BUILD_DIR}\" cd \"${BUILD_DIR}\" echo \"[PYRUN] Now in build dir: $(pwd)\" if [[ -x CsI-WLS ]]; then echo \"[PYRUN] CsI-WLS already built, skipping cmake/cmake --build.\" else echo \"[PYRUN] Configuring with CMake...\" cmake \"../${SRC_DIR}\" echo \"[PYRUN] Building CsI-WLS...\" cmake --build . -- -j\"$(nproc)\" fi if [[ ! -x CsI-WLS ]]; then echo \"[PYRUN] ERROR: CsI-WLS binary not found after build.\" exit 1 fi echo echo \"[PYRUN] Running Python batch script...\" python3 ../source/run_electrons_batch.py echo echo \"[PYRUN] ROOT files under rootOutput_electron/:\" find rootOutput_electron -maxdepth 3 -type f -name '*.root' -print \\ || echo \"[PYRUN] Nessun .root trovato.\" echo echo \"[PYRUN] Done at $(date)\" Lo script va reso eseguibile: chmod +x run_CsI_WLS_electron_batch.sh Infine si definisce il file di submit CsI_WLS_python_electrons.csi : universe = vanilla initialdir = /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2 executable = run_CsI_WLS_electron_batch.sh arguments = container_image = G4_v10.6.3_NOMULTITHREAD.sif request_cpus = 1 request_memory = 2 GB request_disk = 16 GB output = logs/pyCsI_$(ClusterId).$(ProcId).out error = logs/pyCsI_$(ClusterId).$(ProcId).err log = logs/pyCsI_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene con: cd /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2 condor_submit CsI_WLS_python_electrons.csi Quando il job \u00e8 terminato, nella directory build compaiono l\u2019eseguibile CsI-WLS , le macro generate dallo script Python e gli output ROOT. Tutti questi file sono salvati su lustre dentro /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2 . I template completi degli script .sh e dei file di submit .csi utilizzati negli esempi nella sezione Esempi sono presenti al seguente link: https://politecnicobari-my.sharepoint.com/:f:/g/personal/m_cecca1_phd_poliba_it/IgB8X8DaFI3QTaVAEtIk1-7vAavKvy_XEBCvTr6rky2CQf8?e=si0j3J Costruzione di un\u2019immagine Docker e conversione in SIF \u00b6 Gli esempi della sezione Esempi assumono che le immagini .sif siano gi\u00e0 disponibili in una cartella condivisa, gestita dall\u2019utente alice . Questa sezione descrive in modo sintetico come costruire un\u2019immagine Docker con Geant4, ROOT e Python, come convertirla in un\u2019immagine Apptainer/Singularity e come distribuirla agli altri utenti, senza entrare nei dettagli dei singoli comandi di installazione del software all\u2019interno del container. La costruzione di immagini Docker richiede una macchina con Docker installato e accessibile, ad esempio le macchine come tesla02.recas.infn.ba.it . Un utente pu\u00f2 connettersi a quella macchina con le stesse credenziali delle macchine di frontend ui-al9.recas.infn.ba.it , preparare un Dockerfile e costruire l\u2019immagine. Un esempio completo di Dockerfile per un ambiente Ubuntu 24.04 con Geant4, ROOT e Python3 \u00e8 riportato in appendice, nella sezione Dockerfile di esempio per ambiente Geant4/ROOT . Una volta scritto il Dockerfile , l\u2019utente manutentore pu\u00f2 costruire l\u2019immagine con un comando del tipo: docker build -t registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 . In seguito, l\u2019immagine pu\u00f2 essere inviata al registry interno, dove le credenziali sono le stesse del frontend: docker login registry-clustergpu.recas.ba.infn.it docker push registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 La conversione da immagine Docker a immagine Apptainer/Singularity avviene su un nodo dove Apptainer \u00e8 installato e autorizzato a eseguire il comando build . Un esempio di comando \u00e8: apptainer build G4_v10.6.3.sif \\ docker://registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 Il file G4_v10.6.3.sif cos\u00ec generato pu\u00f2 essere copiato o spostato nella directory condivisa delle immagini: mv G4_v10.6.3.sif /lustrehome/alice/apptainer_images/ Dopo questo passaggio, tutti gli utenti (come bob ) possono usare l\u2019immagine nei propri job Condor creando un symlink nella initialdir e impostando container_image al nome del symlink, come mostrato nella sezione Concetti di base: container, Apptainer e HTCondor e negli esempi. Comandi essenziali Docker \u00b6 Non \u00e8 necessario che ogni utente conosca Docker in dettaglio, ma \u00e8 utile riassumere i comandi pi\u00f9 usati nel ciclo di vita di un\u2019immagine. La costruzione da Dockerfile nella directory corrente avviene di solito con: docker build -t nome_immagine:tag . Per visualizzare le immagini locali si pu\u00f2 usare: docker images Per testare interattivamente un\u2019immagine, ad esempio verificando che gli script di ambiente siano corretti, si pu\u00f2 avviare un container con: docker run -it nome_immagine:tag Infine, per taggare e inviare un\u2019immagine verso il registry remoto, si possono usare comandi del tipo: docker tag nome_immagine:tag \\ registry-clustergpu.recas.ba.infn.it/alice/nome_immagine:tag docker push registry-clustergpu.recas.ba.infn.it/alice/nome_immagine:tag Questi comandi vengono eseguiti sulla macchina che ha Docker installato, come tesla02.recas.infn.ba.it . Comandi essenziali Apptainer/Singularity \u00b6 Apptainer viene utilizzato sia per testare manualmente le immagini sia in maniera indiretta tramite HTCondor. Per un test rapido si pu\u00f2 eseguire un comando dentro un\u2019immagine .sif con: apptainer exec G4_v11.3.1.sif geant4-config --version Per aprire una shell interattiva nel container si pu\u00f2 usare: apptainer shell G4_v11.3.1.sif La conversione da immagine Docker a .sif avviene, come gi\u00e0 mostrato, con: apptainer build G4_v11.3.1.sif \\ docker://registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1 I job Condor che usano container_image nascondono questi dettagli, perch\u00e9 \u00e8 il sistema a chiamare internamente Apptainer. Tuttavia, conoscere questi comandi aiuta a testare rapidamente un\u2019immagine su una macchina di frontend prima di costruire i file .csi , come quelli della sezione Esempi . Prospettive: uso di Kubernetes con container \u00b6 (TODO) Questa guida \u00e8 focalizzata sull\u2019uso di HTCondor e Apptainer per eseguire job batch su ReCaS. In prospettiva \u00e8 possibile immaginare un\u2019evoluzione verso l\u2019uso di Kubernetes per l\u2019orchestrazione dei container, ad esempio in contesti in cui il cluster offra un accesso nativo tramite kubectl e un insieme di namespace dedicati alla computazione scientifica. L\u2019idea di base sarebbe quella di mappare i concetti introdotti in questa guida sui costrutti di Kubernetes. Un job HTCondor che esegue un container diventerebbe un oggetto Job di Kubernetes, in cui il campo image dello container spec corrisponde all\u2019immagine Docker/Apptainer usata attualmente in container_image . La nozione di initialdir verrebbe sostituita da un volume persistente (PersistentVolumeClaim) montato nel pod, su cui risiedono sorgenti, script e output. Gli script .sh utilizzati come executable in Condor verrebbero richiamati come command e args del container. In una futura estensione di questa documentazione si potranno fornire esempi espliciti di manifest YAML per Kubernetes che riproducono gli stessi workflow illustrati qui: compilazione di un progetto Geant4 all\u2019interno di un pod, esecuzione di simulazioni con macro, lancio di script Python per la generazione dei run e raccolta dei file ROOT su volumi condivisi. Sar\u00e0 anche possibile discutere l\u2019integrazione con sistemi di job queue pi\u00f9 alti, che inviano richieste sia a HTCondor sia a Kubernetes a seconda del tipo di carico, mantenendo in comune lo stesso set di immagini container e directory di dati su storage condiviso. Appendice \u00b6 Dockerfile di esempio per ambiente Geant4/ROOT \u00b6 In questa sezione \u00e8 riportato un esempio completo di Dockerfile per costruire un\u2019immagine Docker basata su Ubuntu 24.04, con Geant4, ROOT e Python3. Questo file \u00e8 pensato come base da adattare alle esigenze del gruppo, sia per quanto riguarda le versioni dei software, sia per i path di installazione. Il risultato atteso \u00e8 un\u2019immagine che espone gli script di environment /opt/geant4/bin/geant4.sh e /opt/root/bin/thisroot.sh e che pu\u00f2 essere convertita in un file .sif come descritto nella sezione Costruzione di un\u2019immagine Docker e conversione in SIF . FROM ubuntu:24.04 LABEL author=\"marcocecca\" LABEL version=\"G4v11.3.1_Rootv6.36.4_Ubuntu24\" # =========================== # Env Geant4 + ROOT (base) # =========================== ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC ENV G4VERSION=11.3.1 ENV G4INSTALL=/opt/geant4 ENV G4DATA_DIR=$G4INSTALL/share/Geant4/data ENV G4LIB_DIR=$G4INSTALL/lib ENV G4GDMLROOT=$G4INSTALL # ROOT ENV ROOT_VERSION=6.36.04 ENV ROOTSYS=/opt/root # ========================================== # Dipendenze base (Qt5 + OpenGL/X11 + ROOT + Python + Vdt) # ========================================== RUN apt-get update && apt-get install -y --no-install-recommends \\ build-essential \\ cmake \\ git \\ pkg-config \\ ca-certificates \\ wget \\ curl \\ # Geant4 + GDML libxerces-c-dev \\ libexpat1-dev \\ # Qt5 + OpenGL + X11 qtbase5-dev \\ qtbase5-dev-tools \\ qt5-qmake \\ libqt5opengl5-dev \\ libx11-dev \\ libxmu-dev \\ libxi-dev \\ libxrandr-dev \\ libxinerama-dev \\ libxcursor-dev \\ libgl1-mesa-dev \\ libglu1-mesa-dev \\ # runtime Qt/X11 libxkbcommon-x11-0 \\ libfontconfig1 \\ libxrender1 \\ libxcb-icccm4 \\ libxcb-image0 \\ libxcb-keysyms1 \\ libxcb-render-util0 \\ libxcb-xfixes0 \\ libxcb-xinerama0 \\ # dipendenze ROOT libxpm-dev \\ libxft-dev \\ libssl-dev \\ libpcre3-dev \\ libgsl-dev \\ libgraphviz-dev \\ libtbb12 \\ libtbb-dev \\ libvdt-dev \\ # Python per gli script di simulazione python3 \\ python3-numpy \\ python3-matplotlib \\ python3-pip \\ # utility adduser \\ && rm -rf /var/lib/apt/lists/* # ================================ # Mandatory ReCaS: utente reale # ================================ ENV USERNAME=alice ENV USERID=000001 ENV GROUPID=1234 RUN groupadd -g \"$GROUPID\" \"$USERNAME\" && \\ adduser --disabled-password --gecos '' --uid \"$USERID\" --gid \"$GROUPID\" \"$USERNAME\" # ========================================== # Sorgenti Geant4 11.3.1 (da GitLab CERN) # ========================================== RUN mkdir -p /opt/geant4-source /tmp/g4 && \\ cd /tmp/g4 && \\ wget https://gitlab.cern.ch/geant4/geant4/-/archive/v11.3.1/geant4-v11.3.1.tar.gz && \\ tar -xzf geant4-v11.3.1.tar.gz -C /opt/geant4-source --strip-components=1 && \\ rm -rf /tmp/g4 # ========================================== # Build + install Geant4 (dataset inclusi) # ========================================== RUN mkdir -p /opt/geant4-build && cd /opt/geant4-build && \\ cmake ../geant4-source \\ -DCMAKE_INSTALL_PREFIX=${G4INSTALL} \\ -DGEANT4_BUILD_MULTITHREADED=ON \\ -DGEANT4_BUILD_CXXSTD=17 \\ -DGEANT4_INSTALL_DATA=ON \\ -DGEANT4_INSTALL_EXAMPLES=ON \\ -DGEANT4_USE_GDML=ON \\ -DGEANT4_USE_SYSTEM_EXPAT=ON \\ -DGEANT4_USE_SYSTEM_XERCESC=ON \\ -DGEANT4_USE_QT=ON \\ -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/Qt5 \\ -DGEANT4_USE_OPENGL_X11=ON \\ -DGEANT4_USE_XM=OFF \\ -DGEANT4_USE_RAYTRACER_X11=ON && \\ cmake --build . -j\"$(nproc)\" && \\ cmake --install . && \\ rm -rf /opt/geant4-build # ================================================= # Install ROOT (precompiled per Ubuntu 24.04) # ================================================= RUN cd /opt && \\ wget -O root.tar.gz https://root.cern/download/root_v6.36.04.Linux-ubuntu24.04-x86_64-gcc13.3.tar.gz && \\ tar -xzf root.tar.gz && \\ mv root root-${ROOT_VERSION} && \\ ln -s root-${ROOT_VERSION} root && \\ rm root.tar.gz # ================================================= # Linker config + PATH/LD_LIBRARY_PATH globali # ================================================= RUN echo \"${G4LIB_DIR}\" > /etc/ld.so.conf.d/geant4.conf && \\ echo \"${ROOTSYS}/lib\" > /etc/ld.so.conf.d/root.conf && \\ ldconfig ENV PATH=${G4INSTALL}/bin:${ROOTSYS}/bin:${PATH} ENV LD_LIBRARY_PATH=${G4LIB_DIR}:${ROOTSYS}/lib # =========================== # Permessi su /opt/geant4 # =========================== RUN chown -R \"$USERNAME:$GROUPID\" \"$G4INSTALL\" # ====================================================== # EntryPoint: inizializza Geant4 + ROOT nel modo \"giusto\" # ====================================================== RUN printf '%s\\n' \\ '#!/bin/bash' \\ 'set -e' \\ '# --- Geant4 env ---' \\ 'G4_SH=\"${G4INSTALL}/bin/geant4.sh\"' \\ 'if [ -f \"$G4_SH\" ]; then' \\ ' OLD_G4=\"$(pwd)\"' \\ ' cd \"$(dirname \"$G4_SH\")\"' \\ ' . ./geant4.sh' \\ ' cd \"$OLD_G4\"' \\ 'fi' \\ '# --- geant4make (opzionale, per vecchi workflow) ---' \\ 'if [ -f \"${G4INSTALL}/share/Geant4-${G4VERSION}/geant4make/geant4make.sh\" ]; then' \\ ' . \"${G4INSTALL}/share/Geant4-${G4VERSION}/geant4make/geant4make.sh\"' \\ 'fi' \\ '# --- ROOT env ---' \\ 'if [ -f \"/opt/root/bin/thisroot.sh\" ]; then' \\ ' OLD_ROOT=\"$(pwd)\"' \\ ' cd /opt/root' \\ ' . bin/thisroot.sh' \\ ' cd \"$OLD_ROOT\"' \\ 'fi' \\ '# --- Esegui il comando richiesto ---' \\ 'exec \"$@\"' \\ > /usr/local/bin/geant4-entrypoint.sh && \\ chmod +x /usr/local/bin/geant4-entrypoint.sh WORKDIR /home/$USERNAME USER $USERNAME ENTRYPOINT [\"/usr/local/bin/geant4-entrypoint.sh\"] CMD [\"bash\"]","title":"Home"},{"location":"#uso-di-immagini-apptainersingularity-con-htcondor-su-recas","text":"","title":"Uso di immagini Apptainer/Singularity con HTCondor su ReCaS"},{"location":"#sec-introduzione","text":"Questa guida descrive in modo operativo come usare immagini Apptainer/Singularity (file .sif ) insieme a HTCondor sul cluster ReCaS. L\u2019idea di fondo \u00e8 che nel gruppo ci sia almeno un utente \u201cmanutentore\u201d (che chiameremo alice ) che possa costruire immagini Docker su una macchina dedicata (ad esempio una macchina con Docker come tesla02 ), convertirle in immagini Apptainer/Singularity e metterle a disposizione di tutti in una posizione condivisa su lustre. Gli altri utenti (ad esempio bob ) non devono occuparsi della parte Docker: si limitano a usare le immagini .sif gi\u00e0 pronte all\u2019interno dei job Condor, tramite symlink verso la directory condivisa di alice . Nel seguito useremo come esempio un utente chiamato bob per i job Condor, mentre alice rappresenter\u00e0 l\u2019utente manutentore che ospita le immagini condivise. Per rendere gli esempi concreti, si assume che siano gi\u00e0 presenti due immagini Apptainer nella directory condivisa di alice : /lustrehome/alice/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif /lustrehome/alice/apptainer_images/G4_v11.3.1.sif Queste immagini contengono Ubuntu 24.04, Geant4, ROOT, Python3 con numpy e matplotlib. In particolare l\u2019immagine G4_v10.6.3_NOMULTITHREAD.sif ha il multithreading disattivato per Geant4 ed \u00e8 adatta ad applicazioni che richiedono esecuzione single-thread. La sezione Concetti di base: container, Apptainer e HTCondor introduce i concetti di base su container, Apptainer e HTCondor, mentre la sezione Organizzazione delle directory propone una convenzione semplice per organizzare le directory su lustre dal punto di vista di bob . La sezione Esempi illustra cinque esempi completi di utilizzo: test dell\u2019immagine, build dell\u2019esempio B5 di Geant4, run di B5, build+run in un unico job e un caso reale con una simulazione Geant di una tile scintillante tra due piani di fibre WLS (CsI-WLS) e Python. La sezione Costruzione di un\u2019immagine Docker e conversione in SIF mostra come costruire e convertire immagini Docker in .sif , mentre la sezione Prospettive: uso di Kubernetes con container \u00e8 prevista come estensione futura. In Appendice, la sezione Dockerfile di esempio per ambiente Geant4/ROOT contiene un Dockerfile di esempio.","title":"Introduzione"},{"location":"#sec-concetti-base","text":"Prima di entrare negli esempi conviene chiarire cosa si intende per container e come Apptainer interagisce con HTCondor nel contesto del cluster ReCaS. Un container \u00e8 un ambiente software isolato, definito da un\u2019immagine che contiene un sistema operativo minimale (ad esempio Ubuntu), le librerie e le applicazioni necessarie. Quando si avvia un container, il programma viene eseguito con quell\u2019ambiente software, indipendentemente dal sistema operativo del nodo fisico. Nel nostro caso un\u2019immagine .sif contiene Geant4, ROOT, Python e le relative dipendenze, cos\u00ec che un job Condor non deve installare o configurare nulla: trova tutto gi\u00e0 predisposto. Su ReCaS i container sono gestiti da Apptainer (discendente di Singularity), progettato per ambienti HPC multiutente. Un\u2019immagine Apptainer \u00e8 un file in sola lettura; durante il job, Apptainer monta il filesystem dell\u2019immagine e allo stesso tempo monta la directory di lavoro dell\u2019utente, in modo che il programma possa leggere e scrivere i propri file su lustre. Questo approccio \u00e8 pi\u00f9 leggero di una macchina virtuale, perch\u00e9 il kernel del sistema \u00e8 condiviso e si avvia solo lo strato utente. HTCondor si occupa di individuare i worker node disponibili, preparare la directory di lavoro e avviare Apptainer. Dal punto di vista dell\u2019utente, la cosa fondamentale \u00e8 capire il ruolo di tre parametri nel file di submit: initialdir , executable e container_image . La direttiva initialdir indica la directory sul filesystem di lustre che rappresenta la cartella di lavoro del job. Condor monta questa directory nel container come current working directory (CWD), quindi tutto ci\u00f2 che viene scritto in CWD o in sottocartelle relative finisce direttamente in questa directory su lustre. La direttiva executable indica lo script o l\u2019eseguibile che verr\u00e0 lanciato all\u2019interno del container. Deve trovarsi nella initialdir o in una sua sottocartella ed \u00e8 specificato nel file di submit con un percorso relativo. La direttiva container_image indica quale immagine Apptainer usare. I test effettuati sul cluster hanno mostrato un comportamento pratico importante: Condor si aspetta che il valore di container_image sia il nome di un file presente nella initialdir . Per questa ragione, anche se l\u2019immagine \u201creale\u201d vive in una directory centrale, per ogni job conviene creare nella initialdir un symlink locale all\u2019immagine e poi usare nel submit il nome del symlink (ad esempio un symlink a /lustrehome/alice/apptainer_images/immagine.sif ). Un esempio tipico \u00e8 il seguente. Nella directory del job di bob si crea un link all\u2019immagine condivisa di alice : ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif e nel file di submit si scrive: container_image = G4_v11.3.1.sif In questo modo Condor trova il file G4_v11.3.1.sif nella initialdir , avvia Apptainer con quell\u2019immagine e monta la initialdir all\u2019interno del container. Da quel momento in poi lo script executable viene eseguito dentro il container e la directory di lavoro corrisponde alla directory dell\u2019utente su lustre. Gli esempi pratici della sezione Esempi non fanno altro che declinare questo schema base in casi d\u2019uso via via pi\u00f9 complessi.","title":"Concetti di base: container, Apptainer e HTCondor"},{"location":"#sec-organizzazione","text":"Per lavorare in modo ordinato conviene scegliere una convenzione semplice all\u2019interno della propria home su lustre. Nel caso di bob , la directory di riferimento per i job \u00e8 /lustrehome/bob , mentre l\u2019utente manutentore alice usa /lustrehome/alice per ospitare le immagini condivise. Le immagini Apptainer condivise dal manutentore possono essere raccolte in una directory dedicata, ad esempio /lustrehome/alice/apptainer_images . In questa directory si collocano i file .sif che alice ha costruito o recuperato. Nel nostro esempio vi si trovano G4_v11.3.1.sif e G4_v10.6.3_NOMULTITHREAD.sif . Per gli esempi e i job Condor di bob si pu\u00f2 usare una directory condor_tests . All\u2019interno di condor_tests \u00e8 utile creare sottodirectory dedicate per ciascun tipo di job. Ogni directory contiene i file di submit .csi , gli script .sh , un link locale all\u2019immagine .sif (proveniente da /lustrehome/alice/apptainer_images ) e una sottocartella logs/ per gli output di Condor. Questa struttura rende chiaro dove si trova il codice sorgente, dove viene compilato il programma e dove finiscono i file prodotti dai job Condor, seguendo lo schema introdotto in Concetti di base: container, Apptainer e HTCondor e utilizzato in tutti gli esempi successivi.","title":"Organizzazione delle directory"},{"location":"#sec-esempi","text":"In questa sezione sono riportati cinque esempi completi che illustrano come utilizzare immagini Apptainer/Singularity in combinazione con HTCondor. Gli esempi seguono un ordine progressivo, dal test pi\u00f9 semplice fino a un caso realistico con un progetto Geant4 personalizzato: Esempio 1 \u2013 test minimale dell\u2019immagine .sif per verificare la versione di Geant4, ROOT e Python e controllare che il container venga avviato correttamente su un worker node; Esempio 2 \u2013 compilazione dell\u2019esempio Geant4 B5 all\u2019interno del container utilizzando CMake; Esempio 3 \u2013 esecuzione di un binario Geant4 precompilato con una macro, in un job dedicato; Esempio 4 \u2013 compilazione ed esecuzione dell\u2019esempio B5 nello stesso job HTCondor, utile quando si vuole una build \u201cpulita\u201d per ogni run; Esempio 5 \u2013 caso realistico con il progetto CsI-WLS, che prevede build con CMake e un batch di simulazioni pilotato da uno script Python. I template completi degli script .sh e dei file di submit .csi utilizzati negli esempi successivi sono disponibili al seguente link: https://politecnicobari-my.sharepoint.com/:f:/g/personal/m_cecca1_phd_poliba_it/IgB8X8DaFI3QTaVAEtIk1-7vAavKvy_XEBCvTr6rky2CQf8?e=si0j3J","title":"Esempi"},{"location":"#sec-esempio1","text":"Il primo esempio ha lo scopo di verificare che l\u2019immagine G4_v11.3.1.sif funzioni correttamente su un worker node. L\u2019obiettivo \u00e8 sapere su quale nodo gira il job, quali versioni di Geant4, ROOT e Python sono visibili dall\u2019interno del container e se l\u2019ambiente \u00e8 coerente. Si inizia creando la directory del test e una sottocartella per i log: cd /lustrehome/bob mkdir -p condor_tests/test_container/logs cd condor_tests/test_container Si crea un link all\u2019immagine condivisa di alice : ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Lo script test_container.sh , che sar\u00e0 eseguito dentro il container, pu\u00f2 essere definito come segue: #!/bin/bash set -euo pipefail echo \"[TEST] Start: $(date)\" echo \"[TEST] Host: $(hostname)\" echo \"[TEST] User: $(whoami)\" echo \"[TEST] Pwd: $(pwd)\" echo echo \"[TEST] Environment snippet:\" echo \" G4INSTALL=${G4INSTALL:-undefined}\" echo \" G4VERSION=${G4VERSION:-undefined}\" echo \" ROOTSYS=${ROOTSYS:-undefined}\" echo echo \"[TEST] Checking Geant4 / ROOT / Python...\" command -v geant4-config >/dev/null 2>&1 && \\ geant4-config --version || echo \"geant4-config NOT found\" command -v root-config >/dev/null 2>&1 && \\ root-config --version || echo \"root-config NOT found\" command -v python3 >/dev/null 2>&1 && \\ python3 --version || echo \"python3 NOT found\" echo python3 - << 'EOF' import sys, platform print(\"Python:\", sys.version.split()[0]) print(\"Platform:\", platform.platform()) EOF echo echo \"[TEST] Done: $(date)\" Lo script va reso eseguibile: chmod +x test_container.sh Il file di submit test_container.csi specifica la directory iniziale, lo script da eseguire e l\u2019immagine da usare: universe = vanilla initialdir = /lustrehome/bob/condor_tests/test_container executable = test_container.sh arguments = container_image = G4_v11.3.1.sif request_cpus = 1 request_memory = 1 GB request_disk = 4 GB output = logs/test_$(ClusterId).$(ProcId).out error = logs/test_$(ClusterId).$(ProcId).err log = logs/test_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene con: condor_submit test_container.csi Quando il job \u00e8 completato, il file logs/test_...out contiene le informazioni stampate dallo script: il nome del worker node, l\u2019utente, la directory di lavoro, le variabili di ambiente e le versioni dei software principali. Questo conferma che l\u2019immagine \u00e8 correttamente utilizzabile con HTCondor secondo lo schema introdotto in Concetti di base: container, Apptainer e HTCondor .","title":"Esempio 1: test dell'immagine Geant4"},{"location":"#sec-esempio2","text":"Il secondo esempio mostra come compilare l\u2019esempio B5 di Geant4 usando l\u2019immagine G4_v11.3.1.sif . L\u2019obiettivo \u00e8 ottenere l\u2019eseguibile exampleB5 in una directory di build gestita dal job. Si crea la directory del test di build: cd /lustrehome/bob mkdir -p condor_tests/build_B5_11.3.1/logs cd condor_tests/build_B5_11.3.1 Si collega l\u2019immagine condivisa: ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Lo script build_B5_exec.sh viene eseguito dentro il container e si occupa di configurare e compilare il progetto: #!/bin/bash set -euo pipefail echo \"[BUILD] Start: $(date)\" echo \"[BUILD] Host: $(hostname)\" echo \"[BUILD] User: $(whoami)\" echo \"[BUILD] Pwd: $(pwd)\" echo SRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\" BUILD_DIR=\"B5_build_condor\" echo \"[BUILD] SRC_DIR = ${SRC_DIR}\" echo \"[BUILD] BUILD_DIR = ${BUILD_DIR}\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[BUILD] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[BUILD] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi mkdir -p \"${BUILD_DIR}\" cd \"${BUILD_DIR}\" echo \"[BUILD] Now in: $(pwd)\" echo \"[BUILD] Running CMake...\" cmake \"${SRC_DIR}\" echo \"[BUILD] Building...\" cmake --build . -- -j\"$(nproc)\" echo echo \"[BUILD] Done: $(date)\" Lo script viene reso eseguibile: chmod +x build_B5_exec.sh Il file di submit build_B5_11.3.1.csi \u00e8: universe = vanilla initialdir = /lustrehome/bob/condor_tests/build_B5_11.3.1 executable = build_B5_exec.sh arguments = container_image = G4_v11.3.1.sif request_cpus = 4 request_memory = 4 GB request_disk = 8 GB output = logs/build_$(ClusterId).$(ProcId).out error = logs/build_$(ClusterId).$(ProcId).err log = logs/build_$(ClusterId).$(ProcId).log queue 1 Il job si sottomette con: condor_submit build_B5_11.3.1.csi Al termine della compilazione, nella directory /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor si trovano i file di CMake e l\u2019eseguibile exampleB5 . Lo standard output del job contiene i messaggi di CMake e l\u2019esito del build, che verr\u00e0 riutilizzato nella sezione Esempio 3: run dell\u2019esempio B5 .","title":"Esempio 2: build dell\u2019esempio B5 di Geant4"},{"location":"#sec-esempio3","text":"Il terzo esempio riutilizza l\u2019eseguibile exampleB5 compilato in Esempio 2: build dell\u2019esempio B5 di Geant4 e mostra come preparare una directory di run separata, in cui copiare l\u2019eseguibile e le macro e lanciare la simulazione. Si crea la directory per il run: cd /lustrehome/bob mkdir -p condor_tests/run_B5_11.3.1/logs cd condor_tests/run_B5_11.3.1 Si collega l\u2019immagine: ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Si copiano l\u2019eseguibile e la macro run1.mac dalla build: cp /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor/exampleB5 . cp /lustrehome/bob/condor_tests/build_B5_11.3.1/B5_build_condor/run1.mac . Lo script run_B5_exec.sh lancia l\u2019eseguibile con la macro: #!/bin/bash set -euo pipefail if [[ $# -lt 2 ]]; then echo \"Usage: $0 <exec_rel_path> <macro_rel_path>\" echo \"Example: $0 exampleB5 run1.mac\" exit 1 fi EXEC_REL=\"$1\" MACRO_REL=\"$2\" echo \"[RUN] Start: $(date)\" echo \"[RUN] Host: $(hostname)\" echo \"[RUN] User: $(whoami)\" echo \"[RUN] Pwd: $(pwd)\" echo \"[RUN] Exec: ${EXEC_REL}\" echo \"[RUN] Macro: ${MACRO_REL}\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[RUN] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[RUN] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi EXEC_PATH=\"./${EXEC_REL}\" MACRO_PATH=\"./${MACRO_REL}\" if [[ ! -x \"${EXEC_PATH}\" ]]; then echo \"[RUN] ERROR: executable not found or not executable: ${EXEC_PATH}\" exit 1 fi if [[ ! -f \"${MACRO_PATH}\" ]]; then echo \"[RUN] ERROR: macro not found: ${MACRO_PATH}\" exit 1 fi echo \"[RUN] Launching: ${EXEC_PATH} ${MACRO_PATH}\" \"${EXEC_PATH}\" \"${MACRO_PATH}\" echo echo \"[RUN] Done: $(date)\" Lo script va reso eseguibile: chmod +x run_B5_exec.sh Il file di submit run_B5_11.3.1.csi utilizza lo script, l\u2019immagine e specifica le risorse richieste: universe = vanilla initialdir = /lustrehome/bob/condor_tests/run_B5_11.3.1 executable = run_B5_exec.sh arguments = exampleB5 run1.mac container_image = G4_v11.3.1.sif request_cpus = 1 request_memory = 2 GB request_disk = 4 GB output = logs/run_$(ClusterId).$(ProcId).out error = logs/run_$(ClusterId).$(ProcId).err log = logs/run_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene come nei casi precedenti: condor_submit run_B5_11.3.1.csi Gli output prodotti da B5 vengono scritti nella directory /lustrehome/bob/condor_tests/run_B5_11.3.1 , che corrisponde alla directory di lavoro del job dentro il container, e rimangono quindi disponibili all\u2019utente anche dopo la fine dell\u2019esecuzione.","title":"Esempio 3: run dell\u2019esempio B5"},{"location":"#sec-esempio4","text":"In alcune situazioni \u00e8 comodo compilare il codice e far partire subito il run all\u2019interno dello stesso job Condor. Questo permette di avere un singolo file di submit per l\u2019intera catena e di garantire che il run utilizzi esattamente la build prodotta nel job. Per questo esempio si crea una nuova directory: cd /lustrehome/bob mkdir -p condor_tests/build_run_B5_11.3.1/logs cd condor_tests/build_run_B5_11.3.1 Si collega l\u2019immagine Geant4 11.3.1: ln -sf /lustrehome/alice/apptainer_images/G4_v11.3.1.sif \\ G4_v11.3.1.sif Lo script build_run_B5_exec.sh esegue prima la compilazione dell\u2019esempio B5 e subito dopo l\u2019eseguibile con una macro di esempio: #!/bin/bash set -euo pipefail echo \"[BUILD+RUN] Start: $(date)\" echo \"[BUILD+RUN] Host: $(hostname)\" echo \"[BUILD+RUN] User: $(whoami)\" echo \"[BUILD+RUN] Pwd: $(pwd)\" echo SRC_DIR=\"/opt/geant4/share/Geant4/examples/basic/B5\" BUILD_DIR=\"B5_build_condor\" MACRO=\"${SRC_DIR}/run1.mac\" echo \"[BUILD+RUN] SRC_DIR = ${SRC_DIR}\" echo \"[BUILD+RUN] BUILD_DIR = ${BUILD_DIR}\" echo \"[BUILD+RUN] MACRO = ${MACRO}\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[BUILD+RUN] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[BUILD+RUN] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi mkdir -p \"${BUILD_DIR}\" cd \"${BUILD_DIR}\" echo \"[BUILD+RUN] Now in: $(pwd)\" if [[ -x exampleB5 ]]; then echo \"[BUILD+RUN] exampleB5 already built, skipping CMake/cmake --build.\" else echo \"[BUILD+RUN] Running CMake...\" cmake \"${SRC_DIR}\" echo \"[BUILD+RUN] Building...\" cmake --build . -- -j\"$(nproc)\" fi if [[ ! -x exampleB5 ]]; then echo \"[BUILD+RUN] ERROR: build failed, exampleB5 not found.\" exit 1 fi echo echo \"[BUILD+RUN] Running exampleB5 with macro: ${MACRO}\" ./exampleB5 \"${MACRO}\" echo echo \"[BUILD+RUN] ROOT files under $(pwd):\" find . -maxdepth 3 -type f -name '*.root' -print || \\ echo \"[BUILD+RUN] Nessun .root trovato.\" echo echo \"[BUILD+RUN] Done: $(date)\" Lo script viene reso eseguibile: chmod +x build_run_B5_exec.sh Il file di submit build_run_B5_11.3.1.csi \u00e8 il seguente: universe = vanilla initialdir = /lustrehome/bob/condor_tests/build_run_B5_11.3.1 executable = build_run_B5_exec.sh arguments = container_image = G4_v11.3.1.sif request_cpus = 4 request_memory = 4 GB request_disk = 8 GB output = logs/build_run_$(ClusterId).$(ProcId).out error = logs/build_run_$(ClusterId).$(ProcId).err log = logs/build_run_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene con: condor_submit build_run_B5_11.3.1.csi Alla conclusione del job, la directory B5_build_condor contiene sia i file generati da CMake, sia l\u2019eseguibile exampleB5 , sia i file di output del run (ad esempio file ROOT). Tutti questi file risiedono in /lustrehome/bob/condor_tests/build_run_B5_11.3.1/B5_build_condor e sono quindi accessibili per analisi successive.","title":"Esempio 4: build e run di B5 in un unico job"},{"location":"#sec-esempio5","text":"L\u2019ultimo esempio mostra una situazione pi\u00f9 vicina a un caso reale, in cui un\u2019applicazione Geant4 custom (CsI-WLS) viene compilata da sorgente con CMake e poi eseguita molte volte con parametri diversi, gestiti da uno script Python. Per questo caso si sfrutta l\u2019immagine G4_v10.6.3_NOMULTITHREAD.sif , che contiene Geant4 10.6.3 senza multithreading, ROOT 6.36.4 e Python3 con le librerie principali. Si assume che il progetto CsI-WLS esista gi\u00e0 in una directory di sviluppo, ad esempio /lustrehome/bob/ADAPT/simulation/CsI-WLS_v1.2.2 , che contiene una sottodirectory src con i file CMake e il codice. Per rendere il job auto-contenuto dentro condor_tests si copia solo la directory src : cd /lustrehome/bob/condor_tests mkdir -p CsI-WLS_v1.2.2 cp -r /lustrehome/bob/ADAPT/simulation/CsI-WLS_v1.2.2/src CsI-WLS_v1.2.2/ cd CsI-WLS_v1.2.2 mkdir -p logs Si crea il link all\u2019immagine senza multithreading di alice : ln -sf /lustrehome/alice/apptainer_images/G4_v10.6.3_NOMULTITHREAD.sif \\ G4_v10.6.3_NOMULTITHREAD.sif Nella directory source (nel testo LaTeX originale si alternano source / src ; qui assumiamo che lo script Python sia nella directory con i sorgenti) si definisce lo script Python che si aspetta di essere eseguito da dentro build (dove esiste ./CsI-WLS ). Questo script genera un certo numero di macro, ognuna con un seed diverso, e per ciascuna macro lancia l\u2019eseguibile: import numpy as np import os import matplotlib.pyplot as plt # non usato, ma non da fastidio DIR_MAC = \"./mac_electron\" DIR_ROOT = \"rootOutput_electron\" N_EVENTS = 10 subdir = \"random_rectangular_source_200keV\" nfiles = 50 for k in range(nfiles): seed1, seed2 = np.random.randint(0, 2**32, size=2) mac = (f\"{DIR_MAC}/{subdir}/\" f\"random_rectangular_source_electron_ene_\" f\"200keV_ly1_n{k}.mac\") root = mac.replace(DIR_MAC, DIR_ROOT).replace(\".mac\", \"\") os.makedirs(os.path.dirname(mac), exist_ok=True) os.makedirs(os.path.dirname(root), exist_ok=True) with open(mac, \"w\") as f: f.write( \"/run/initialize\\n\" \"/tracking/verbose 0\\n\" \"/gps/particle e-\\n\" \"/gps/position 0 0 0 mm\\n\" \"/gps/direction 0 0 -1\\n\" f\"/random/setSeeds [{seed1} {seed2}]\\n\" \"/gps/pos/type Plane\\n\" \"/gps/pos/shape Rectangle\\n\" \"/gps/pos/halfx 220 mm\\n\" \"/gps/pos/halfy 220 mm\\n\" \"/gps/ene/mono 200 keV\\n\" f\"/RunManager/NameOfOutputFile {root}\\n\" f\"/run/beamOn {N_EVENTS}\\n\" ) os.system(f\"./CsI-WLS {mac}\") print(f\"\\n {N_EVENTS*nfiles} eventi salvati in \" f\"{DIR_MAC}/{subdir} e simulati con ./CsI-WLS\") Questo file pu\u00f2 essere salvato come src/run_electrons_batch.py (o source/run_electrons_batch.py , a seconda della struttura del progetto) all\u2019interno della copia di CsI-WLS sotto condor_tests . Nella root della directory CsI-WLS_v1.2.2 si definisce lo script run_CsI_WLS_electron_batch.sh , che compila il progetto nella directory build e poi lancia lo script Python: #!/bin/bash set -euo pipefail echo \"[PYRUN] Start: $(date)\" echo \"[PYRUN] Host: $(hostname)\" echo \"[PYRUN] User: $(whoami)\" echo \"[PYRUN] Pwd: $(pwd)\" echo if [[ -f /opt/geant4/bin/geant4.sh ]]; then echo \"[PYRUN] Sourcing Geant4...\" source /opt/geant4/bin/geant4.sh fi if [[ -f /opt/root/bin/thisroot.sh ]]; then echo \"[PYRUN] Sourcing ROOT...\" source /opt/root/bin/thisroot.sh fi BUILD_DIR=\"build\" SRC_DIR=\"source\" mkdir -p \"${BUILD_DIR}\" cd \"${BUILD_DIR}\" echo \"[PYRUN] Now in build dir: $(pwd)\" if [[ -x CsI-WLS ]]; then echo \"[PYRUN] CsI-WLS already built, skipping cmake/cmake --build.\" else echo \"[PYRUN] Configuring with CMake...\" cmake \"../${SRC_DIR}\" echo \"[PYRUN] Building CsI-WLS...\" cmake --build . -- -j\"$(nproc)\" fi if [[ ! -x CsI-WLS ]]; then echo \"[PYRUN] ERROR: CsI-WLS binary not found after build.\" exit 1 fi echo echo \"[PYRUN] Running Python batch script...\" python3 ../source/run_electrons_batch.py echo echo \"[PYRUN] ROOT files under rootOutput_electron/:\" find rootOutput_electron -maxdepth 3 -type f -name '*.root' -print \\ || echo \"[PYRUN] Nessun .root trovato.\" echo echo \"[PYRUN] Done at $(date)\" Lo script va reso eseguibile: chmod +x run_CsI_WLS_electron_batch.sh Infine si definisce il file di submit CsI_WLS_python_electrons.csi : universe = vanilla initialdir = /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2 executable = run_CsI_WLS_electron_batch.sh arguments = container_image = G4_v10.6.3_NOMULTITHREAD.sif request_cpus = 1 request_memory = 2 GB request_disk = 16 GB output = logs/pyCsI_$(ClusterId).$(ProcId).out error = logs/pyCsI_$(ClusterId).$(ProcId).err log = logs/pyCsI_$(ClusterId).$(ProcId).log queue 1 La sottomissione avviene con: cd /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2 condor_submit CsI_WLS_python_electrons.csi Quando il job \u00e8 terminato, nella directory build compaiono l\u2019eseguibile CsI-WLS , le macro generate dallo script Python e gli output ROOT. Tutti questi file sono salvati su lustre dentro /lustrehome/bob/condor_tests/CsI-WLS_v1.2.2 . I template completi degli script .sh e dei file di submit .csi utilizzati negli esempi nella sezione Esempi sono presenti al seguente link: https://politecnicobari-my.sharepoint.com/:f:/g/personal/m_cecca1_phd_poliba_it/IgB8X8DaFI3QTaVAEtIk1-7vAavKvy_XEBCvTr6rky2CQf8?e=si0j3J","title":"Esempio 5: progetto CsI-WLS con Python"},{"location":"#sec-docker-sif","text":"Gli esempi della sezione Esempi assumono che le immagini .sif siano gi\u00e0 disponibili in una cartella condivisa, gestita dall\u2019utente alice . Questa sezione descrive in modo sintetico come costruire un\u2019immagine Docker con Geant4, ROOT e Python, come convertirla in un\u2019immagine Apptainer/Singularity e come distribuirla agli altri utenti, senza entrare nei dettagli dei singoli comandi di installazione del software all\u2019interno del container. La costruzione di immagini Docker richiede una macchina con Docker installato e accessibile, ad esempio le macchine come tesla02.recas.infn.ba.it . Un utente pu\u00f2 connettersi a quella macchina con le stesse credenziali delle macchine di frontend ui-al9.recas.infn.ba.it , preparare un Dockerfile e costruire l\u2019immagine. Un esempio completo di Dockerfile per un ambiente Ubuntu 24.04 con Geant4, ROOT e Python3 \u00e8 riportato in appendice, nella sezione Dockerfile di esempio per ambiente Geant4/ROOT . Una volta scritto il Dockerfile , l\u2019utente manutentore pu\u00f2 costruire l\u2019immagine con un comando del tipo: docker build -t registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 . In seguito, l\u2019immagine pu\u00f2 essere inviata al registry interno, dove le credenziali sono le stesse del frontend: docker login registry-clustergpu.recas.ba.infn.it docker push registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 La conversione da immagine Docker a immagine Apptainer/Singularity avviene su un nodo dove Apptainer \u00e8 installato e autorizzato a eseguire il comando build . Un esempio di comando \u00e8: apptainer build G4_v10.6.3.sif \\ docker://registry-clustergpu.recas.ba.infn.it/alice/geant4:10.6.3 Il file G4_v10.6.3.sif cos\u00ec generato pu\u00f2 essere copiato o spostato nella directory condivisa delle immagini: mv G4_v10.6.3.sif /lustrehome/alice/apptainer_images/ Dopo questo passaggio, tutti gli utenti (come bob ) possono usare l\u2019immagine nei propri job Condor creando un symlink nella initialdir e impostando container_image al nome del symlink, come mostrato nella sezione Concetti di base: container, Apptainer e HTCondor e negli esempi.","title":"Costruzione di un\u2019immagine Docker e conversione in SIF"},{"location":"#comandi-essenziali-docker","text":"Non \u00e8 necessario che ogni utente conosca Docker in dettaglio, ma \u00e8 utile riassumere i comandi pi\u00f9 usati nel ciclo di vita di un\u2019immagine. La costruzione da Dockerfile nella directory corrente avviene di solito con: docker build -t nome_immagine:tag . Per visualizzare le immagini locali si pu\u00f2 usare: docker images Per testare interattivamente un\u2019immagine, ad esempio verificando che gli script di ambiente siano corretti, si pu\u00f2 avviare un container con: docker run -it nome_immagine:tag Infine, per taggare e inviare un\u2019immagine verso il registry remoto, si possono usare comandi del tipo: docker tag nome_immagine:tag \\ registry-clustergpu.recas.ba.infn.it/alice/nome_immagine:tag docker push registry-clustergpu.recas.ba.infn.it/alice/nome_immagine:tag Questi comandi vengono eseguiti sulla macchina che ha Docker installato, come tesla02.recas.infn.ba.it .","title":"Comandi essenziali Docker"},{"location":"#comandi-essenziali-apptainersingularity","text":"Apptainer viene utilizzato sia per testare manualmente le immagini sia in maniera indiretta tramite HTCondor. Per un test rapido si pu\u00f2 eseguire un comando dentro un\u2019immagine .sif con: apptainer exec G4_v11.3.1.sif geant4-config --version Per aprire una shell interattiva nel container si pu\u00f2 usare: apptainer shell G4_v11.3.1.sif La conversione da immagine Docker a .sif avviene, come gi\u00e0 mostrato, con: apptainer build G4_v11.3.1.sif \\ docker://registry-clustergpu.recas.ba.infn.it/alice/geant4:11.3.1 I job Condor che usano container_image nascondono questi dettagli, perch\u00e9 \u00e8 il sistema a chiamare internamente Apptainer. Tuttavia, conoscere questi comandi aiuta a testare rapidamente un\u2019immagine su una macchina di frontend prima di costruire i file .csi , come quelli della sezione Esempi .","title":"Comandi essenziali Apptainer/Singularity"},{"location":"#sec-kubernetes","text":"(TODO) Questa guida \u00e8 focalizzata sull\u2019uso di HTCondor e Apptainer per eseguire job batch su ReCaS. In prospettiva \u00e8 possibile immaginare un\u2019evoluzione verso l\u2019uso di Kubernetes per l\u2019orchestrazione dei container, ad esempio in contesti in cui il cluster offra un accesso nativo tramite kubectl e un insieme di namespace dedicati alla computazione scientifica. L\u2019idea di base sarebbe quella di mappare i concetti introdotti in questa guida sui costrutti di Kubernetes. Un job HTCondor che esegue un container diventerebbe un oggetto Job di Kubernetes, in cui il campo image dello container spec corrisponde all\u2019immagine Docker/Apptainer usata attualmente in container_image . La nozione di initialdir verrebbe sostituita da un volume persistente (PersistentVolumeClaim) montato nel pod, su cui risiedono sorgenti, script e output. Gli script .sh utilizzati come executable in Condor verrebbero richiamati come command e args del container. In una futura estensione di questa documentazione si potranno fornire esempi espliciti di manifest YAML per Kubernetes che riproducono gli stessi workflow illustrati qui: compilazione di un progetto Geant4 all\u2019interno di un pod, esecuzione di simulazioni con macro, lancio di script Python per la generazione dei run e raccolta dei file ROOT su volumi condivisi. Sar\u00e0 anche possibile discutere l\u2019integrazione con sistemi di job queue pi\u00f9 alti, che inviano richieste sia a HTCondor sia a Kubernetes a seconda del tipo di carico, mantenendo in comune lo stesso set di immagini container e directory di dati su storage condiviso.","title":"Prospettive: uso di Kubernetes con container"},{"location":"#appendice","text":"","title":"Appendice"},{"location":"#sec-dockerfile-esempio","text":"In questa sezione \u00e8 riportato un esempio completo di Dockerfile per costruire un\u2019immagine Docker basata su Ubuntu 24.04, con Geant4, ROOT e Python3. Questo file \u00e8 pensato come base da adattare alle esigenze del gruppo, sia per quanto riguarda le versioni dei software, sia per i path di installazione. Il risultato atteso \u00e8 un\u2019immagine che espone gli script di environment /opt/geant4/bin/geant4.sh e /opt/root/bin/thisroot.sh e che pu\u00f2 essere convertita in un file .sif come descritto nella sezione Costruzione di un\u2019immagine Docker e conversione in SIF . FROM ubuntu:24.04 LABEL author=\"marcocecca\" LABEL version=\"G4v11.3.1_Rootv6.36.4_Ubuntu24\" # =========================== # Env Geant4 + ROOT (base) # =========================== ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC ENV G4VERSION=11.3.1 ENV G4INSTALL=/opt/geant4 ENV G4DATA_DIR=$G4INSTALL/share/Geant4/data ENV G4LIB_DIR=$G4INSTALL/lib ENV G4GDMLROOT=$G4INSTALL # ROOT ENV ROOT_VERSION=6.36.04 ENV ROOTSYS=/opt/root # ========================================== # Dipendenze base (Qt5 + OpenGL/X11 + ROOT + Python + Vdt) # ========================================== RUN apt-get update && apt-get install -y --no-install-recommends \\ build-essential \\ cmake \\ git \\ pkg-config \\ ca-certificates \\ wget \\ curl \\ # Geant4 + GDML libxerces-c-dev \\ libexpat1-dev \\ # Qt5 + OpenGL + X11 qtbase5-dev \\ qtbase5-dev-tools \\ qt5-qmake \\ libqt5opengl5-dev \\ libx11-dev \\ libxmu-dev \\ libxi-dev \\ libxrandr-dev \\ libxinerama-dev \\ libxcursor-dev \\ libgl1-mesa-dev \\ libglu1-mesa-dev \\ # runtime Qt/X11 libxkbcommon-x11-0 \\ libfontconfig1 \\ libxrender1 \\ libxcb-icccm4 \\ libxcb-image0 \\ libxcb-keysyms1 \\ libxcb-render-util0 \\ libxcb-xfixes0 \\ libxcb-xinerama0 \\ # dipendenze ROOT libxpm-dev \\ libxft-dev \\ libssl-dev \\ libpcre3-dev \\ libgsl-dev \\ libgraphviz-dev \\ libtbb12 \\ libtbb-dev \\ libvdt-dev \\ # Python per gli script di simulazione python3 \\ python3-numpy \\ python3-matplotlib \\ python3-pip \\ # utility adduser \\ && rm -rf /var/lib/apt/lists/* # ================================ # Mandatory ReCaS: utente reale # ================================ ENV USERNAME=alice ENV USERID=000001 ENV GROUPID=1234 RUN groupadd -g \"$GROUPID\" \"$USERNAME\" && \\ adduser --disabled-password --gecos '' --uid \"$USERID\" --gid \"$GROUPID\" \"$USERNAME\" # ========================================== # Sorgenti Geant4 11.3.1 (da GitLab CERN) # ========================================== RUN mkdir -p /opt/geant4-source /tmp/g4 && \\ cd /tmp/g4 && \\ wget https://gitlab.cern.ch/geant4/geant4/-/archive/v11.3.1/geant4-v11.3.1.tar.gz && \\ tar -xzf geant4-v11.3.1.tar.gz -C /opt/geant4-source --strip-components=1 && \\ rm -rf /tmp/g4 # ========================================== # Build + install Geant4 (dataset inclusi) # ========================================== RUN mkdir -p /opt/geant4-build && cd /opt/geant4-build && \\ cmake ../geant4-source \\ -DCMAKE_INSTALL_PREFIX=${G4INSTALL} \\ -DGEANT4_BUILD_MULTITHREADED=ON \\ -DGEANT4_BUILD_CXXSTD=17 \\ -DGEANT4_INSTALL_DATA=ON \\ -DGEANT4_INSTALL_EXAMPLES=ON \\ -DGEANT4_USE_GDML=ON \\ -DGEANT4_USE_SYSTEM_EXPAT=ON \\ -DGEANT4_USE_SYSTEM_XERCESC=ON \\ -DGEANT4_USE_QT=ON \\ -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/Qt5 \\ -DGEANT4_USE_OPENGL_X11=ON \\ -DGEANT4_USE_XM=OFF \\ -DGEANT4_USE_RAYTRACER_X11=ON && \\ cmake --build . -j\"$(nproc)\" && \\ cmake --install . && \\ rm -rf /opt/geant4-build # ================================================= # Install ROOT (precompiled per Ubuntu 24.04) # ================================================= RUN cd /opt && \\ wget -O root.tar.gz https://root.cern/download/root_v6.36.04.Linux-ubuntu24.04-x86_64-gcc13.3.tar.gz && \\ tar -xzf root.tar.gz && \\ mv root root-${ROOT_VERSION} && \\ ln -s root-${ROOT_VERSION} root && \\ rm root.tar.gz # ================================================= # Linker config + PATH/LD_LIBRARY_PATH globali # ================================================= RUN echo \"${G4LIB_DIR}\" > /etc/ld.so.conf.d/geant4.conf && \\ echo \"${ROOTSYS}/lib\" > /etc/ld.so.conf.d/root.conf && \\ ldconfig ENV PATH=${G4INSTALL}/bin:${ROOTSYS}/bin:${PATH} ENV LD_LIBRARY_PATH=${G4LIB_DIR}:${ROOTSYS}/lib # =========================== # Permessi su /opt/geant4 # =========================== RUN chown -R \"$USERNAME:$GROUPID\" \"$G4INSTALL\" # ====================================================== # EntryPoint: inizializza Geant4 + ROOT nel modo \"giusto\" # ====================================================== RUN printf '%s\\n' \\ '#!/bin/bash' \\ 'set -e' \\ '# --- Geant4 env ---' \\ 'G4_SH=\"${G4INSTALL}/bin/geant4.sh\"' \\ 'if [ -f \"$G4_SH\" ]; then' \\ ' OLD_G4=\"$(pwd)\"' \\ ' cd \"$(dirname \"$G4_SH\")\"' \\ ' . ./geant4.sh' \\ ' cd \"$OLD_G4\"' \\ 'fi' \\ '# --- geant4make (opzionale, per vecchi workflow) ---' \\ 'if [ -f \"${G4INSTALL}/share/Geant4-${G4VERSION}/geant4make/geant4make.sh\" ]; then' \\ ' . \"${G4INSTALL}/share/Geant4-${G4VERSION}/geant4make/geant4make.sh\"' \\ 'fi' \\ '# --- ROOT env ---' \\ 'if [ -f \"/opt/root/bin/thisroot.sh\" ]; then' \\ ' OLD_ROOT=\"$(pwd)\"' \\ ' cd /opt/root' \\ ' . bin/thisroot.sh' \\ ' cd \"$OLD_ROOT\"' \\ 'fi' \\ '# --- Esegui il comando richiesto ---' \\ 'exec \"$@\"' \\ > /usr/local/bin/geant4-entrypoint.sh && \\ chmod +x /usr/local/bin/geant4-entrypoint.sh WORKDIR /home/$USERNAME USER $USERNAME ENTRYPOINT [\"/usr/local/bin/geant4-entrypoint.sh\"] CMD [\"bash\"]","title":"Dockerfile di esempio per ambiente Geant4/ROOT"}]}